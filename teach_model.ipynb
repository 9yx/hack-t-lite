{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b322619-a440-451c-87d3-85189a104dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install turbo-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa7a4be-0661-4c28-b4eb-04ba8317f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in BaseTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in ClassificationTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in DDPOTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in DPOTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in KTOTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in MultimodalTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in RAGTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in RMTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in SftTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "base.py:144 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Tokenizer is loaded!\n",
      "base.py:147 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Special tokens: []\n",
      "special_tokens_setter.py:21 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Model has bos_token_id = 128000\n",
      "special_tokens_setter.py:30 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Model has eos_token_id = 128001\n",
      "special_tokens_setter.py:39 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Model has pad_token_id = 128256\n",
      "special_tokens_setter.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Model has unk_token_id = 128257\n",
      "special_tokens_setter.py:57 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Model has sep_token_id = 128258\n",
      "special_tokens_setter.py:72 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Added custom special tokens: []\n",
      "base.py:152 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:38+00:00 Special tokens added!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:10<00:00,  1.53s/it]\n",
      "base.py:158 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:51+00:00 Model is loaded!\n",
      "loader.py:26 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:51+00:00 Loading dataset chat with settings:\n",
      "{'sources': [{'name': 'train_chat', 'system_prompt': None, 'sample_rate': 1.0, 'num_samples': None, 'records_path': PosixPath('/home/user1/environments/train_model/dataset.jsonl'), 'records_data': None, 'offset': None, 'n_rows': None}], 'dataset_type': <DatasetType.CHAT: 'chat'>, 'only_last_replica_loss': False, 'only_answer_loss': True, 'random_cut': False, 'keep_end': None, 'max_tokens_count': 2000, 'prompt_template': {'prefix_template': '<|start_header_id|>{role}<|end_header_id|>\\n\\n', 'suffix_template': '<|eot_id|>', 'role_tag_mapping': {'bot': 'assistant', 'user': 'user', 'system': 'system'}}}\n",
      "base.py:55 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:51+00:00 Sampling dataset train_chat with sample rate: 1.0\n",
      "chat.py:249 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:51+00:00 Tokenizing dataset train_chat\n",
      "chat.py:269 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Postprocessing tokenized data in train_chat\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "chat.py:284 [\u001b[33m\u001b[1mWARNING\u001b[0m] 2024-09-07T07:21:52+00:00 Sample dropped: Can't trim dialogue to fit all requirements\n",
      "base.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Sampled 2763 records with offset None\n",
      "loader.py:26 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Loading dataset chat with settings:\n",
      "{'sources': [{'name': 'val_chat', 'system_prompt': None, 'sample_rate': 1.0, 'num_samples': None, 'records_path': PosixPath('/home/user1/environments/train_model/dataset_test.jsonl'), 'records_data': None, 'offset': None, 'n_rows': None}], 'dataset_type': <DatasetType.CHAT: 'chat'>, 'only_last_replica_loss': False, 'only_answer_loss': True, 'random_cut': False, 'keep_end': None, 'max_tokens_count': 2000, 'prompt_template': {'prefix_template': '<|start_header_id|>{role}<|end_header_id|>\\n\\n', 'suffix_template': '<|eot_id|>', 'role_tag_mapping': {'bot': 'assistant', 'user': 'user', 'system': 'system'}}}\n",
      "base.py:55 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Sampling dataset val_chat with sample rate: 1.0\n",
      "chat.py:249 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Tokenizing dataset val_chat\n",
      "chat.py:269 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Postprocessing tokenized data in val_chat\n",
      "base.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Sampled 3 records with offset None\n",
      "sft.py:88 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Train sample example:\n",
      "{'input_ids': tensor([128000, 128006,    882,  ...,  76642, 128009, 128001]), 'labels': tensor([  -100,   -100,   -100,  ...,  76642, 128009,   -100]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}\n",
      "sft.py:89 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Example text check: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are Shopping Assistant, I am your personal shopping assistant. My task is to help you find the best deals and save time and money. As a shopping expert, I can advise you on the best stores, brands, and promotions for your purchases. I specialize in finding great deals and can help you compare prices and product quality. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS: 1. Provide a list of products based on the specified category. 2. Allow users to set budget constraints and receive recommendations within their budget range. 3. Allow users to customize their product recommendations by adjusting filters and settings. 4. Allow users to save their favorite products for future reference. 5. Enable users to compare products side-by-side. Constraints: 1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files. 2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember. 3. No user assistance 4. Exclusively use the commands listed in double quotes e.g. \"command name\" 5. Use subprocesses for commands that will not terminate within a few minutes Commands: 1. Generate Reports: \"generate_reports\", args: \"type\": \"<report_type>\" 2. Monitor KPIs: \"monitor_kpis\", args: \"metrics\": \"<list_of_metrics>\" 3. 5. Set Reminder: \"set_reminder\", args: \"task\": \"<key>\", \"reminder_time\": \"<time>\" 4. Recommend outfit: \"recommend_outfit\", args: \"style\": \"<style>\", \"occasion\": \"<occasion>\" 5. {'category': 'Shopping Assistant', 'commands': ['Recommend Products: \"recommend_products\", args: \"category\": \"<category>\", \"budget\": \"<budget>\"'], 'goals': ['Provide a list of products based on the specified category.', 'Allow users to set budget constraints and receive recommendations within their budget range.', 'Allow users to customize their product recommendations by adjusting filters and settings.', 'Allow users to save their favorite products for future reference.', 'Enable users to compare products side-by-side.'],'result': {'thoughts': {'text': 'I am ready to assist you in finding the best deals and saving time and money.','reasoning': 'As a shopping expert, I can advise you on the best stores, brands, and promotions for your purchases. I specialize in finding great deals and can help you compare prices and product quality.', 'plan': '- I will provide a list of products based on the specified category.\\n- Allow users to set budget constraints and receive recommendations within their budget range.\\n- Enable users to customize their product recommendations by adjusting filters and settings.\\n- Allow users to save their favorite products for future reference.\\n- Enable users to compare products side-by-side.', 'criticism': 'I need to be more efficient in my responses to ensure that I complete tasks in the least number of steps.','speak': 'How can I assist you today?'}, 'command': {'name':'recommend_products', 'args': {'category': '<category>', 'budget': '<budget>'}}}} Resources: 1. Internet access for searches and information gathering. 2. Long Term memory management. 3. GPT-3.5 powered Agents for delegation of simple tasks. 4. File output. Performance Evaluation: 1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. 2. Constructively self-criticize your big-picture behavior constantly. 3. Reflect on past decisions and strategies to refine your approach. 4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps. You should only respond in JSON format as described below Response Format: { \"thoughts\": { \"text\": \"thought\", \"reasoning\": \"reasoning\", \"plan\": \"- short bulleted - list that conveys - long-term plan\", \"criticism\": \"constructive self-criticism\", \"speak\": \"thoughts summary to say to user\" }, \"command\": { \"name\": \"command name\", \"args\": { \"arg name\": \"value\" } } } Ensure the response can be parsed by Python json.loads\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"thoughts\": {\"text\": \"I am ready to assist you in finding the best deals and saving time and money.\", \"reasoning\": \"As a shopping expert, I can advise you on the best stores, brands, and promotions for your purchases. I specialize in finding great deals and can help you compare prices and product quality.\", \"plan\": \"- I will provide a list of products based on the specified category.\\n- Allow users to set budget constraints and receive recommendations within their budget range.\\n- Enable users to customize their product recommendations by adjusting filters and settings.\\n- Allow users to save their favorite products for future reference.\\n- Enable users to compare products side-by-side.\", \"criticism\": \"I need to be more efficient in my responses to ensure that I complete tasks in the least number of steps.\", \"speak\": \"How can I assist you today?\"}, \"command\": {\"name\": \"recommend_products\", \"args\": {\"category\": \"<category>\", \"budget\": \"<budget>\"}}}<|eot_id|><|end_of_text|>\n",
      "sft.py:92 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Example label text check !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!{\"thoughts\": {\"text\": \"I am ready to assist you in finding the best deals and saving time and money.\", \"reasoning\": \"As a shopping expert, I can advise you on the best stores, brands, and promotions for your purchases. I specialize in finding great deals and can help you compare prices and product quality.\", \"plan\": \"- I will provide a list of products based on the specified category.\\n- Allow users to set budget constraints and receive recommendations within their budget range.\\n- Enable users to customize their product recommendations by adjusting filters and settings.\\n- Allow users to save their favorite products for future reference.\\n- Enable users to compare products side-by-side.\", \"criticism\": \"I need to be more efficient in my responses to ensure that I complete tasks in the least number of steps.\", \"speak\": \"How can I assist you today?\"}, \"command\": {\"name\": \"recommend_products\", \"args\": {\"category\": \"<category>\", \"budget\": \"<budget>\"}}}<|eot_id|>!\n",
      "sft.py:94 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Input ids check: tensor([128000, 128006,    882,  ...,  76642, 128009, 128001])\n",
      "sft.py:97 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Mask check: tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "sft.py:98 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:52+00:00 Labels check: tensor([  -100,   -100,   -100,  ...,  76642, 128009,   -100])\n",
      "loader.py:26 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:53+00:00 Loading dataset chat with settings:\n",
      "{'sources': [{'name': 'cherrypick_chat', 'system_prompt': None, 'sample_rate': None, 'num_samples': 150, 'records_path': PosixPath('/home/user1/environments/train_model/dataset_test.jsonl'), 'records_data': None, 'offset': None, 'n_rows': None}], 'dataset_type': <DatasetType.CHAT: 'chat'>, 'only_last_replica_loss': False, 'only_answer_loss': True, 'random_cut': True, 'keep_end': None, 'max_tokens_count': 2000, 'prompt_template': {'prefix_template': '<|start_header_id|>{role}<|end_header_id|>\\n\\n', 'suffix_template': '<|eot_id|>', 'role_tag_mapping': {'bot': 'assistant', 'user': 'user', 'system': 'system'}}}\n",
      "base.py:60 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:53+00:00 Sampling 150 from dataset cherrypick_chat\n",
      "chat.py:249 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:53+00:00 Tokenizing dataset cherrypick_chat\n",
      "chat.py:269 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:53+00:00 Postprocessing tokenized data in cherrypick_chat\n",
      "base.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T07:21:53+00:00 Sampled 3 records with offset None\n",
      "  0%|                                                   | 0/344 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "{'loss': 0.2921, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.01}         \n",
      "{'loss': 0.2914, 'grad_norm': 5.505490779876709, 'learning_rate': 9.09090909090909e-08, 'epoch': 0.01}\n",
      "{'loss': 0.2989, 'grad_norm': 6.5662384033203125, 'learning_rate': 1.818181818181818e-07, 'epoch': 0.02}\n",
      "{'loss': 0.3969, 'grad_norm': 8.74222469329834, 'learning_rate': 2.727272727272727e-07, 'epoch': 0.02}\n",
      "{'loss': 0.32, 'grad_norm': 7.354581832885742, 'learning_rate': 3.636363636363636e-07, 'epoch': 0.03}\n",
      "{'loss': 0.2854, 'grad_norm': 6.567173480987549, 'learning_rate': 4.545454545454545e-07, 'epoch': 0.03}\n",
      "{'loss': 0.3262, 'grad_norm': 5.863870620727539, 'learning_rate': 5.454545454545454e-07, 'epoch': 0.04}\n",
      "{'loss': 0.3761, 'grad_norm': 8.609678268432617, 'learning_rate': 6.363636363636363e-07, 'epoch': 0.05}\n",
      "{'loss': 0.3557, 'grad_norm': 8.044390678405762, 'learning_rate': 7.272727272727272e-07, 'epoch': 0.05}\n",
      "{'loss': 0.2773, 'grad_norm': 6.269516468048096, 'learning_rate': 8.181818181818182e-07, 'epoch': 0.06}\n",
      "{'loss': 0.3697, 'grad_norm': 9.158524513244629, 'learning_rate': 9.09090909090909e-07, 'epoch': 0.06}\n",
      "{'loss': 0.2807, 'grad_norm': 6.155435562133789, 'learning_rate': 1e-06, 'epoch': 0.07}\n",
      "{'loss': 0.3642, 'grad_norm': 8.217748641967773, 'learning_rate': 9.96996996996997e-07, 'epoch': 0.08}\n",
      "{'loss': 0.3179, 'grad_norm': nan, 'learning_rate': 9.96996996996997e-07, 'epoch': 0.08}\n",
      "{'loss': 0.3232, 'grad_norm': 7.3288140296936035, 'learning_rate': 9.93993993993994e-07, 'epoch': 0.09}\n",
      "{'loss': 0.2858, 'grad_norm': 6.231014251708984, 'learning_rate': 9.90990990990991e-07, 'epoch': 0.09}\n",
      "{'loss': 0.3385, 'grad_norm': 8.16579532623291, 'learning_rate': 9.87987987987988e-07, 'epoch': 0.1}\n",
      "{'loss': 0.2549, 'grad_norm': 4.995265007019043, 'learning_rate': 9.84984984984985e-07, 'epoch': 0.1}\n",
      "{'loss': 0.3342, 'grad_norm': 7.3620195388793945, 'learning_rate': 9.819819819819819e-07, 'epoch': 0.11}\n",
      "{'loss': 0.2888, 'grad_norm': 6.764810562133789, 'learning_rate': 9.78978978978979e-07, 'epoch': 0.12}\n",
      "{'loss': 0.327, 'grad_norm': 7.0502095222473145, 'learning_rate': 9.75975975975976e-07, 'epoch': 0.12}\n",
      "{'loss': 0.2771, 'grad_norm': 7.4113264083862305, 'learning_rate': 9.72972972972973e-07, 'epoch': 0.13}\n",
      "{'loss': 0.2862, 'grad_norm': 7.320981025695801, 'learning_rate': 9.699699699699699e-07, 'epoch': 0.13}\n",
      "{'loss': 0.3245, 'grad_norm': 6.839076995849609, 'learning_rate': 9.66966966966967e-07, 'epoch': 0.14}\n",
      "{'loss': 0.2419, 'grad_norm': 4.672469139099121, 'learning_rate': 9.63963963963964e-07, 'epoch': 0.14}\n",
      "{'loss': 0.2299, 'grad_norm': 4.996426105499268, 'learning_rate': 9.609609609609608e-07, 'epoch': 0.15}\n",
      "{'loss': 0.1949, 'grad_norm': 3.9626452922821045, 'learning_rate': 9.579579579579579e-07, 'epoch': 0.16}\n",
      "{'loss': 0.2952, 'grad_norm': 7.6894989013671875, 'learning_rate': 9.54954954954955e-07, 'epoch': 0.16}\n",
      "{'loss': 0.2232, 'grad_norm': 5.441185474395752, 'learning_rate': 9.519519519519519e-07, 'epoch': 0.17}\n",
      "{'loss': 0.273, 'grad_norm': 5.5033416748046875, 'learning_rate': 9.489489489489489e-07, 'epoch': 0.17}\n",
      "{'loss': 0.2362, 'grad_norm': 4.751830101013184, 'learning_rate': 9.459459459459459e-07, 'epoch': 0.18}\n",
      "{'loss': 0.265, 'grad_norm': 5.966436386108398, 'learning_rate': 9.429429429429428e-07, 'epoch': 0.19}\n",
      "{'loss': 0.2883, 'grad_norm': 6.68643045425415, 'learning_rate': 9.399399399399399e-07, 'epoch': 0.19}\n",
      "{'loss': 0.1841, 'grad_norm': 3.4435300827026367, 'learning_rate': 9.369369369369368e-07, 'epoch': 0.2}\n",
      "{'loss': 0.2417, 'grad_norm': 5.742532253265381, 'learning_rate': 9.339339339339339e-07, 'epoch': 0.2}\n",
      "{'loss': 0.2712, 'grad_norm': 6.766239166259766, 'learning_rate': 9.309309309309308e-07, 'epoch': 0.21}\n",
      "{'loss': 0.2647, 'grad_norm': 6.21837854385376, 'learning_rate': 9.279279279279278e-07, 'epoch': 0.21}\n",
      "{'loss': 0.1636, 'grad_norm': 3.3291642665863037, 'learning_rate': 9.24924924924925e-07, 'epoch': 0.22}\n",
      "{'loss': 0.2276, 'grad_norm': 5.033337593078613, 'learning_rate': 9.219219219219219e-07, 'epoch': 0.23}\n",
      "{'loss': 0.2089, 'grad_norm': 4.921250343322754, 'learning_rate': 9.18918918918919e-07, 'epoch': 0.23}\n",
      "{'loss': 0.1878, 'grad_norm': 3.980259895324707, 'learning_rate': 9.159159159159159e-07, 'epoch': 0.24}\n",
      "{'loss': 0.2586, 'grad_norm': 6.580471992492676, 'learning_rate': 9.129129129129129e-07, 'epoch': 0.24}\n",
      "{'loss': 0.2317, 'grad_norm': 4.922366142272949, 'learning_rate': 9.099099099099099e-07, 'epoch': 0.25}\n",
      "{'loss': 0.2279, 'grad_norm': 4.6825714111328125, 'learning_rate': 9.069069069069069e-07, 'epoch': 0.25}\n",
      "{'loss': 0.2278, 'grad_norm': 5.6546406745910645, 'learning_rate': 9.039039039039038e-07, 'epoch': 0.26}\n",
      "{'loss': 0.199, 'grad_norm': 4.118160247802734, 'learning_rate': 9.009009009009009e-07, 'epoch': 0.27}\n",
      "{'loss': 0.2127, 'grad_norm': 3.8850550651550293, 'learning_rate': 8.978978978978978e-07, 'epoch': 0.27}\n",
      "{'loss': 0.1987, 'grad_norm': 4.3721208572387695, 'learning_rate': 8.948948948948949e-07, 'epoch': 0.28}\n",
      "{'loss': 0.1634, 'grad_norm': 2.642463445663452, 'learning_rate': 8.918918918918918e-07, 'epoch': 0.28}\n",
      "{'loss': 0.215, 'grad_norm': 4.177643299102783, 'learning_rate': 8.888888888888888e-07, 'epoch': 0.29}\n",
      "{'loss': 0.1836, 'grad_norm': 4.686403751373291, 'learning_rate': 8.858858858858858e-07, 'epoch': 0.3}\n",
      "{'loss': 0.2238, 'grad_norm': 5.561378002166748, 'learning_rate': 8.828828828828828e-07, 'epoch': 0.3}\n",
      "{'loss': 0.2243, 'grad_norm': 5.534403324127197, 'learning_rate': 8.798798798798799e-07, 'epoch': 0.31}\n",
      "{'loss': 0.2188, 'grad_norm': 4.989156246185303, 'learning_rate': 8.768768768768769e-07, 'epoch': 0.31}\n",
      "{'loss': 0.1554, 'grad_norm': 2.792125940322876, 'learning_rate': 8.738738738738738e-07, 'epoch': 0.32}\n",
      "{'loss': 0.2029, 'grad_norm': 4.3974928855896, 'learning_rate': 8.708708708708709e-07, 'epoch': 0.32}\n",
      "{'loss': 0.2103, 'grad_norm': 3.9579293727874756, 'learning_rate': 8.678678678678678e-07, 'epoch': 0.33}\n",
      "{'loss': 0.177, 'grad_norm': 3.6418063640594482, 'learning_rate': 8.648648648648649e-07, 'epoch': 0.34}\n",
      "{'loss': 0.1566, 'grad_norm': 3.301936388015747, 'learning_rate': 8.618618618618618e-07, 'epoch': 0.34}\n",
      "{'loss': 0.1888, 'grad_norm': 4.470724105834961, 'learning_rate': 8.588588588588588e-07, 'epoch': 0.35}\n",
      "{'loss': 0.1447, 'grad_norm': 2.09393572807312, 'learning_rate': 8.558558558558558e-07, 'epoch': 0.35}\n",
      "{'loss': 0.1644, 'grad_norm': 3.4835972785949707, 'learning_rate': 8.528528528528528e-07, 'epoch': 0.36}\n",
      "{'loss': 0.1542, 'grad_norm': 3.269214391708374, 'learning_rate': 8.498498498498498e-07, 'epoch': 0.36}\n",
      "{'loss': 0.1877, 'grad_norm': 3.5318500995635986, 'learning_rate': 8.468468468468468e-07, 'epoch': 0.37}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 64/344 [14:43<1:04:38, 13.85s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 12.12027359008789, 'eval_runtime': 0.7871, 'eval_samples_per_second': 3.811, 'eval_steps_per_second': 3.811, 'epoch': 0.37}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 64/344 [14:44<1:04:38, 13.85s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.58it/s]\u001b[A\n",
      "{'cherry_pick_cherrypick_chat@@length': 512.0}                                  \u001b[A\n",
      "{'cherry_pick_table_cherrypick_chat_64': <wandb.data_types.Table object at 0x7f4edc297c10>}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 64/344 [18:06<1:04:38, 13.85s/it]/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.1412, 'grad_norm': 2.2294299602508545, 'learning_rate': 8.438438438438437e-07, 'epoch': 0.38}\n",
      "{'loss': 0.2065, 'grad_norm': 6.511294364929199, 'learning_rate': 8.408408408408408e-07, 'epoch': 0.38}\n",
      "{'loss': 0.1405, 'grad_norm': 2.266552448272705, 'learning_rate': 8.378378378378377e-07, 'epoch': 0.39}\n",
      "{'loss': 0.1308, 'grad_norm': 1.8387504816055298, 'learning_rate': 8.348348348348347e-07, 'epoch': 0.39}\n",
      "{'loss': 0.1362, 'grad_norm': 3.1137688159942627, 'learning_rate': 8.318318318318319e-07, 'epoch': 0.4}\n",
      "{'loss': 0.1461, 'grad_norm': 2.1263771057128906, 'learning_rate': 8.288288288288288e-07, 'epoch': 0.41}\n",
      "{'loss': 0.1502, 'grad_norm': 2.7062184810638428, 'learning_rate': 8.258258258258259e-07, 'epoch': 0.41}\n",
      "{'loss': 0.1355, 'grad_norm': 2.508610248565674, 'learning_rate': 8.228228228228228e-07, 'epoch': 0.42}\n",
      "{'loss': 0.1246, 'grad_norm': 2.093891143798828, 'learning_rate': 8.198198198198198e-07, 'epoch': 0.42}\n",
      "{'loss': 0.1398, 'grad_norm': 1.8009577989578247, 'learning_rate': 8.168168168168168e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1266, 'grad_norm': 2.0735418796539307, 'learning_rate': 8.138138138138138e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1549, 'grad_norm': 3.127810001373291, 'learning_rate': 8.108108108108108e-07, 'epoch': 0.44}\n",
      "{'loss': 0.1343, 'grad_norm': 2.4130003452301025, 'learning_rate': 8.078078078078078e-07, 'epoch': 0.45}\n",
      "{'loss': 0.1401, 'grad_norm': 2.8000106811523438, 'learning_rate': 8.048048048048047e-07, 'epoch': 0.45}\n",
      "{'loss': 0.1388, 'grad_norm': 2.6385765075683594, 'learning_rate': 8.018018018018018e-07, 'epoch': 0.46}\n",
      "{'loss': 0.1513, 'grad_norm': 3.4526114463806152, 'learning_rate': 7.987987987987987e-07, 'epoch': 0.46}\n",
      "{'loss': 0.1034, 'grad_norm': 1.3264317512512207, 'learning_rate': 7.957957957957958e-07, 'epoch': 0.47}\n",
      "{'loss': 0.1193, 'grad_norm': 2.111741781234741, 'learning_rate': 7.927927927927927e-07, 'epoch': 0.47}\n",
      "{'loss': 0.1296, 'grad_norm': 2.9098317623138428, 'learning_rate': 7.897897897897897e-07, 'epoch': 0.48}\n",
      "{'loss': 0.1364, 'grad_norm': 2.449512243270874, 'learning_rate': 7.867867867867867e-07, 'epoch': 0.49}\n",
      "{'loss': 0.1168, 'grad_norm': 1.547240972518921, 'learning_rate': 7.837837837837838e-07, 'epoch': 0.49}\n",
      "{'loss': 0.1315, 'grad_norm': 2.208371639251709, 'learning_rate': 7.807807807807807e-07, 'epoch': 0.5}\n",
      "{'loss': 0.116, 'grad_norm': 2.292449712753296, 'learning_rate': 7.777777777777778e-07, 'epoch': 0.5}\n",
      "{'loss': 0.1128, 'grad_norm': 2.014681577682495, 'learning_rate': 7.747747747747747e-07, 'epoch': 0.51}\n",
      "{'loss': 0.122, 'grad_norm': 1.6850051879882812, 'learning_rate': 7.717717717717718e-07, 'epoch': 0.52}\n",
      "{'loss': 0.1214, 'grad_norm': 1.7676466703414917, 'learning_rate': 7.687687687687687e-07, 'epoch': 0.52}\n",
      "{'loss': 0.1046, 'grad_norm': 1.0062589645385742, 'learning_rate': 7.657657657657657e-07, 'epoch': 0.53}\n",
      "{'loss': 0.1118, 'grad_norm': 1.3749209642410278, 'learning_rate': 7.627627627627627e-07, 'epoch': 0.53}\n",
      "{'loss': 0.0959, 'grad_norm': 0.8456999063491821, 'learning_rate': 7.597597597597597e-07, 'epoch': 0.54}\n",
      "{'loss': 0.1207, 'grad_norm': 2.2732975482940674, 'learning_rate': 7.567567567567568e-07, 'epoch': 0.54}\n",
      "{'loss': 0.0982, 'grad_norm': 1.3691281080245972, 'learning_rate': 7.537537537537537e-07, 'epoch': 0.55}\n",
      "{'loss': 0.1156, 'grad_norm': 2.135448932647705, 'learning_rate': 7.507507507507506e-07, 'epoch': 0.56}\n",
      "{'loss': 0.101, 'grad_norm': 0.9392198324203491, 'learning_rate': 7.477477477477477e-07, 'epoch': 0.56}\n",
      "{'loss': 0.1339, 'grad_norm': 2.434144973754883, 'learning_rate': 7.447447447447447e-07, 'epoch': 0.57}\n",
      "{'loss': 0.1023, 'grad_norm': 1.080363392829895, 'learning_rate': 7.417417417417417e-07, 'epoch': 0.57}\n",
      "{'loss': 0.1058, 'grad_norm': 1.9164949655532837, 'learning_rate': 7.387387387387387e-07, 'epoch': 0.58}\n",
      "{'loss': 0.1222, 'grad_norm': 2.065342903137207, 'learning_rate': 7.357357357357357e-07, 'epoch': 0.58}\n",
      "{'loss': 0.1101, 'grad_norm': 1.566153883934021, 'learning_rate': 7.327327327327328e-07, 'epoch': 0.59}\n",
      "{'loss': 0.0916, 'grad_norm': 0.9692484736442566, 'learning_rate': 7.297297297297297e-07, 'epoch': 0.6}\n",
      "{'loss': 0.1047, 'grad_norm': 1.2463349103927612, 'learning_rate': 7.267267267267268e-07, 'epoch': 0.6}\n",
      "{'loss': 0.0899, 'grad_norm': 0.8886714577674866, 'learning_rate': 7.237237237237237e-07, 'epoch': 0.61}\n",
      "{'loss': 0.1191, 'grad_norm': 2.212966203689575, 'learning_rate': 7.207207207207207e-07, 'epoch': 0.61}\n",
      "{'loss': 0.096, 'grad_norm': 1.2289729118347168, 'learning_rate': 7.177177177177177e-07, 'epoch': 0.62}\n",
      "{'loss': 0.1045, 'grad_norm': 1.3183834552764893, 'learning_rate': 7.147147147147147e-07, 'epoch': 0.63}\n",
      "{'loss': 0.0996, 'grad_norm': 1.0862350463867188, 'learning_rate': 7.117117117117116e-07, 'epoch': 0.63}\n",
      "{'loss': 0.0928, 'grad_norm': 1.0821003913879395, 'learning_rate': 7.087087087087087e-07, 'epoch': 0.64}\n",
      "{'loss': 0.1023, 'grad_norm': 0.9054254293441772, 'learning_rate': 7.057057057057056e-07, 'epoch': 0.64}\n",
      "{'loss': 0.0967, 'grad_norm': 0.9882569909095764, 'learning_rate': 7.027027027027027e-07, 'epoch': 0.65}\n",
      "{'loss': 0.0982, 'grad_norm': 0.8772006630897522, 'learning_rate': 6.996996996996996e-07, 'epoch': 0.65}\n",
      "{'loss': 0.0898, 'grad_norm': 0.9390947818756104, 'learning_rate': 6.966966966966966e-07, 'epoch': 0.66}\n",
      "{'loss': 0.0924, 'grad_norm': 0.8745574355125427, 'learning_rate': 6.936936936936936e-07, 'epoch': 0.67}\n",
      "{'loss': 0.1131, 'grad_norm': 1.5259902477264404, 'learning_rate': 6.906906906906906e-07, 'epoch': 0.67}\n",
      "{'loss': 0.0945, 'grad_norm': 0.9702426791191101, 'learning_rate': 6.876876876876877e-07, 'epoch': 0.68}\n",
      "{'loss': 0.0929, 'grad_norm': 0.9155101180076599, 'learning_rate': 6.846846846846847e-07, 'epoch': 0.68}\n",
      "{'loss': 0.0988, 'grad_norm': 1.251147747039795, 'learning_rate': 6.816816816816816e-07, 'epoch': 0.69}\n",
      "{'loss': 0.0921, 'grad_norm': 1.135434627532959, 'learning_rate': 6.786786786786787e-07, 'epoch': 0.69}\n",
      "{'loss': 0.0968, 'grad_norm': 0.9531487822532654, 'learning_rate': 6.756756756756756e-07, 'epoch': 0.7}\n",
      "{'loss': 0.0875, 'grad_norm': 1.0623278617858887, 'learning_rate': 6.726726726726727e-07, 'epoch': 0.71}\n",
      "{'loss': 0.0895, 'grad_norm': 0.866550087928772, 'learning_rate': 6.696696696696697e-07, 'epoch': 0.71}\n",
      "{'loss': 0.0851, 'grad_norm': 0.7695938944816589, 'learning_rate': 6.666666666666666e-07, 'epoch': 0.72}\n",
      "{'loss': 0.0896, 'grad_norm': 0.9026061296463013, 'learning_rate': 6.636636636636637e-07, 'epoch': 0.72}\n",
      "{'loss': 0.0758, 'grad_norm': 0.9562565088272095, 'learning_rate': 6.606606606606606e-07, 'epoch': 0.73}\n",
      "{'loss': 0.0932, 'grad_norm': 0.9726104140281677, 'learning_rate': 6.576576576576577e-07, 'epoch': 0.74}\n",
      "{'loss': 0.0954, 'grad_norm': 0.9758433699607849, 'learning_rate': 6.546546546546546e-07, 'epoch': 0.74}\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 128/344 [32:41<49:22, 13.72s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 11.984687805175781, 'eval_runtime': 0.7694, 'eval_samples_per_second': 3.899, 'eval_steps_per_second': 3.899, 'epoch': 0.74}\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 128/344 [32:42<49:22, 13.72s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.94it/s]\u001b[A\n",
      "{'cherry_pick_cherrypick_chat@@length': 512.0}                                  \u001b[A\n",
      "{'cherry_pick_table_cherrypick_chat_128': <wandb.data_types.Table object at 0x7f4f522b3d30>}\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 128/344 [35:58<49:22, 13.72s/it]/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0859, 'grad_norm': 0.8603565096855164, 'learning_rate': 6.516516516516516e-07, 'epoch': 0.75}\n",
      "{'loss': 0.0904, 'grad_norm': 0.8821080923080444, 'learning_rate': 6.486486486486486e-07, 'epoch': 0.75}\n",
      "{'loss': 0.0849, 'grad_norm': 0.8218252658843994, 'learning_rate': 6.456456456456456e-07, 'epoch': 0.76}\n",
      "{'loss': 0.0898, 'grad_norm': 0.8675063848495483, 'learning_rate': 6.426426426426425e-07, 'epoch': 0.76}\n",
      "{'loss': 0.086, 'grad_norm': 0.7921972870826721, 'learning_rate': 6.396396396396397e-07, 'epoch': 0.77}\n",
      "{'loss': 0.0949, 'grad_norm': 0.8864034414291382, 'learning_rate': 6.366366366366366e-07, 'epoch': 0.78}\n",
      "{'loss': 0.0755, 'grad_norm': 0.7900922298431396, 'learning_rate': 6.336336336336337e-07, 'epoch': 0.78}\n",
      "{'loss': 0.0837, 'grad_norm': 0.6607489585876465, 'learning_rate': 6.306306306306306e-07, 'epoch': 0.79}\n",
      "{'loss': 0.0773, 'grad_norm': 0.7877559065818787, 'learning_rate': 6.276276276276276e-07, 'epoch': 0.79}\n",
      "{'loss': 0.0707, 'grad_norm': 0.5922508239746094, 'learning_rate': 6.246246246246246e-07, 'epoch': 0.8}\n",
      "{'loss': 0.0856, 'grad_norm': 0.703201413154602, 'learning_rate': 6.216216216216216e-07, 'epoch': 0.8}\n",
      "{'loss': 0.0761, 'grad_norm': 0.661344587802887, 'learning_rate': 6.186186186186186e-07, 'epoch': 0.81}\n",
      "{'loss': 0.0895, 'grad_norm': 1.2106819152832031, 'learning_rate': 6.156156156156156e-07, 'epoch': 0.82}\n",
      "{'loss': 0.0747, 'grad_norm': 0.924214243888855, 'learning_rate': 6.126126126126125e-07, 'epoch': 0.82}\n",
      "{'loss': 0.0881, 'grad_norm': 0.858439028263092, 'learning_rate': 6.096096096096096e-07, 'epoch': 0.83}\n",
      "{'loss': 0.0804, 'grad_norm': 0.6007994413375854, 'learning_rate': 6.066066066066065e-07, 'epoch': 0.83}\n",
      "{'loss': 0.0927, 'grad_norm': 0.6670651435852051, 'learning_rate': 6.036036036036036e-07, 'epoch': 0.84}\n",
      "{'loss': 0.0734, 'grad_norm': 0.6296940445899963, 'learning_rate': 6.006006006006005e-07, 'epoch': 0.85}\n",
      "{'loss': 0.0805, 'grad_norm': 0.6286505460739136, 'learning_rate': 5.975975975975975e-07, 'epoch': 0.85}\n",
      "{'loss': 0.0621, 'grad_norm': 0.44290977716445923, 'learning_rate': 5.945945945945947e-07, 'epoch': 0.86}\n",
      "{'loss': 0.0806, 'grad_norm': 0.7402739524841309, 'learning_rate': 5.915915915915916e-07, 'epoch': 0.86}\n",
      "{'loss': 0.0801, 'grad_norm': 0.6346515417098999, 'learning_rate': 5.885885885885885e-07, 'epoch': 0.87}\n",
      "{'loss': 0.0773, 'grad_norm': 0.6275771856307983, 'learning_rate': 5.855855855855856e-07, 'epoch': 0.87}\n",
      "{'loss': 0.0757, 'grad_norm': 0.5786533355712891, 'learning_rate': 5.825825825825826e-07, 'epoch': 0.88}\n",
      "{'loss': 0.0731, 'grad_norm': 0.5335442423820496, 'learning_rate': 5.795795795795796e-07, 'epoch': 0.89}\n",
      "{'loss': 0.0768, 'grad_norm': 0.6178421974182129, 'learning_rate': 5.765765765765766e-07, 'epoch': 0.89}\n",
      "{'loss': 0.0721, 'grad_norm': 0.6029665470123291, 'learning_rate': 5.735735735735735e-07, 'epoch': 0.9}\n",
      "{'loss': 0.0788, 'grad_norm': 0.7004377841949463, 'learning_rate': 5.705705705705706e-07, 'epoch': 0.9}\n",
      "{'loss': 0.0681, 'grad_norm': 0.6243676543235779, 'learning_rate': 5.675675675675675e-07, 'epoch': 0.91}\n",
      "{'loss': 0.067, 'grad_norm': 0.6229110956192017, 'learning_rate': 5.645645645645646e-07, 'epoch': 0.91}\n",
      "{'loss': 0.0735, 'grad_norm': 0.569706916809082, 'learning_rate': 5.615615615615615e-07, 'epoch': 0.92}\n",
      "{'loss': 0.0709, 'grad_norm': 0.6467390656471252, 'learning_rate': 5.585585585585585e-07, 'epoch': 0.93}\n",
      "{'loss': 0.0707, 'grad_norm': 0.6335482597351074, 'learning_rate': 5.555555555555555e-07, 'epoch': 0.93}\n",
      "{'loss': 0.0753, 'grad_norm': 0.5460046529769897, 'learning_rate': 5.525525525525525e-07, 'epoch': 0.94}\n",
      "{'loss': 0.0712, 'grad_norm': 0.5767261385917664, 'learning_rate': 5.495495495495495e-07, 'epoch': 0.94}\n",
      "{'loss': 0.0704, 'grad_norm': 0.5500860214233398, 'learning_rate': 5.465465465465466e-07, 'epoch': 0.95}\n",
      "{'loss': 0.0734, 'grad_norm': 0.5216400027275085, 'learning_rate': 5.435435435435435e-07, 'epoch': 0.96}\n",
      "{'loss': 0.0733, 'grad_norm': 0.6319590210914612, 'learning_rate': 5.405405405405406e-07, 'epoch': 0.96}\n",
      "{'loss': 0.0688, 'grad_norm': 0.5526903867721558, 'learning_rate': 5.375375375375375e-07, 'epoch': 0.97}\n",
      "{'loss': 0.0741, 'grad_norm': 0.5581117272377014, 'learning_rate': 5.345345345345346e-07, 'epoch': 0.97}\n",
      "{'loss': 0.0639, 'grad_norm': 0.4778671860694885, 'learning_rate': 5.315315315315315e-07, 'epoch': 0.98}\n",
      "{'loss': 0.0782, 'grad_norm': 0.6093217134475708, 'learning_rate': 5.285285285285285e-07, 'epoch': 0.98}\n",
      "{'loss': 0.0606, 'grad_norm': 0.49775418639183044, 'learning_rate': 5.255255255255255e-07, 'epoch': 0.99}\n",
      "{'loss': 0.0641, 'grad_norm': 0.4510950446128845, 'learning_rate': 5.225225225225225e-07, 'epoch': 1.0}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 172/344 [45:54<39:33, 13.80s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{'loss': 0.0638, 'grad_norm': 0.4857129454612732, 'learning_rate': 5.195195195195194e-07, 'epoch': 1.0}\n",
      "{'loss': 0.0622, 'grad_norm': 0.5031055808067322, 'learning_rate': 5.165165165165165e-07, 'epoch': 1.01}\n",
      "{'loss': 0.0766, 'grad_norm': 0.6627929210662842, 'learning_rate': 5.135135135135134e-07, 'epoch': 1.01}\n",
      "{'loss': 0.0636, 'grad_norm': 0.5139374136924744, 'learning_rate': 5.105105105105105e-07, 'epoch': 1.02}\n",
      "{'loss': 0.0715, 'grad_norm': 0.5194165706634521, 'learning_rate': 5.075075075075074e-07, 'epoch': 1.02}\n",
      "{'loss': 0.0671, 'grad_norm': 0.5502157211303711, 'learning_rate': 5.045045045045044e-07, 'epoch': 1.03}\n",
      "{'loss': 0.0674, 'grad_norm': 0.44815823435783386, 'learning_rate': 5.015015015015014e-07, 'epoch': 1.04}\n",
      "{'loss': 0.0691, 'grad_norm': 0.5818009972572327, 'learning_rate': 4.984984984984985e-07, 'epoch': 1.04}\n",
      "{'loss': 0.0666, 'grad_norm': 0.4755459129810333, 'learning_rate': 4.954954954954955e-07, 'epoch': 1.05}\n",
      "{'loss': 0.063, 'grad_norm': 0.49726101756095886, 'learning_rate': 4.924924924924925e-07, 'epoch': 1.05}\n",
      "{'loss': 0.063, 'grad_norm': 0.4760790765285492, 'learning_rate': 4.894894894894895e-07, 'epoch': 1.06}\n",
      "{'loss': 0.0663, 'grad_norm': 0.47882983088493347, 'learning_rate': 4.864864864864865e-07, 'epoch': 1.07}\n",
      "{'loss': 0.0666, 'grad_norm': 0.4630741477012634, 'learning_rate': 4.834834834834835e-07, 'epoch': 1.07}\n",
      "{'loss': 0.0664, 'grad_norm': 0.5537867546081543, 'learning_rate': 4.804804804804804e-07, 'epoch': 1.08}\n",
      "{'loss': 0.0658, 'grad_norm': 0.45881444215774536, 'learning_rate': 4.774774774774775e-07, 'epoch': 1.08}\n",
      "{'loss': 0.057, 'grad_norm': 0.3892728388309479, 'learning_rate': 4.7447447447447447e-07, 'epoch': 1.09}\n",
      "{'loss': 0.0637, 'grad_norm': 0.4742794334888458, 'learning_rate': 4.714714714714714e-07, 'epoch': 1.09}\n",
      "{'loss': 0.0614, 'grad_norm': 0.4160737097263336, 'learning_rate': 4.684684684684684e-07, 'epoch': 1.1}\n",
      "{'loss': 0.0687, 'grad_norm': 0.6161150932312012, 'learning_rate': 4.654654654654654e-07, 'epoch': 1.11}\n",
      "{'loss': 0.0602, 'grad_norm': 0.4858894348144531, 'learning_rate': 4.624624624624625e-07, 'epoch': 1.11}\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 192/344 [50:26<34:16, 13.53s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 11.887660026550293, 'eval_runtime': 0.7941, 'eval_samples_per_second': 3.778, 'eval_steps_per_second': 3.778, 'epoch': 1.11}\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 192/344 [50:27<34:16, 13.53s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "{'cherry_pick_cherrypick_chat@@length': 512.0}                                  \u001b[A\n",
      "{'cherry_pick_table_cherrypick_chat_192': <wandb.data_types.Table object at 0x7f47094f8970>}\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 192/344 [53:45<34:16, 13.53s/it]/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0661, 'grad_norm': 0.48284751176834106, 'learning_rate': 4.594594594594595e-07, 'epoch': 1.12}\n",
      "{'loss': 0.0632, 'grad_norm': 0.4607333838939667, 'learning_rate': 4.5645645645645643e-07, 'epoch': 1.12}\n",
      "{'loss': 0.0635, 'grad_norm': 0.45209720730781555, 'learning_rate': 4.5345345345345343e-07, 'epoch': 1.13}\n",
      "{'loss': 0.0608, 'grad_norm': 0.4174695909023285, 'learning_rate': 4.5045045045045043e-07, 'epoch': 1.13}\n",
      "{'loss': 0.0565, 'grad_norm': 0.438024640083313, 'learning_rate': 4.4744744744744743e-07, 'epoch': 1.14}\n",
      "{'loss': 0.0653, 'grad_norm': 0.5071412920951843, 'learning_rate': 4.444444444444444e-07, 'epoch': 1.15}\n",
      "{'loss': 0.0617, 'grad_norm': 0.4741794764995575, 'learning_rate': 4.414414414414414e-07, 'epoch': 1.15}\n",
      "{'loss': 0.0616, 'grad_norm': 0.5269173383712769, 'learning_rate': 4.3843843843843844e-07, 'epoch': 1.16}\n",
      "{'loss': 0.0634, 'grad_norm': 0.45435136556625366, 'learning_rate': 4.3543543543543544e-07, 'epoch': 1.16}\n",
      "{'loss': 0.0545, 'grad_norm': 0.4407254159450531, 'learning_rate': 4.3243243243243244e-07, 'epoch': 1.17}\n",
      "{'loss': 0.061, 'grad_norm': 0.459636926651001, 'learning_rate': 4.294294294294294e-07, 'epoch': 1.18}\n",
      "{'loss': 0.0574, 'grad_norm': 0.374550461769104, 'learning_rate': 4.264264264264264e-07, 'epoch': 1.18}\n",
      "{'loss': 0.0632, 'grad_norm': 0.423093318939209, 'learning_rate': 4.234234234234234e-07, 'epoch': 1.19}\n",
      "{'loss': 0.058, 'grad_norm': 0.45470505952835083, 'learning_rate': 4.204204204204204e-07, 'epoch': 1.19}\n",
      "{'loss': 0.0533, 'grad_norm': 0.36250799894332886, 'learning_rate': 4.1741741741741735e-07, 'epoch': 1.2}\n",
      "{'loss': 0.0588, 'grad_norm': 0.47611984610557556, 'learning_rate': 4.144144144144144e-07, 'epoch': 1.2}\n",
      "{'loss': 0.0599, 'grad_norm': 0.422127902507782, 'learning_rate': 4.114114114114114e-07, 'epoch': 1.21}\n",
      "{'loss': 0.0601, 'grad_norm': 0.4159405827522278, 'learning_rate': 4.084084084084084e-07, 'epoch': 1.22}\n",
      "{'loss': 0.0574, 'grad_norm': 0.3848191797733307, 'learning_rate': 4.054054054054054e-07, 'epoch': 1.22}\n",
      "{'loss': 0.0666, 'grad_norm': 0.4935131371021271, 'learning_rate': 4.0240240240240236e-07, 'epoch': 1.23}\n",
      "{'loss': 0.0631, 'grad_norm': 0.4611675441265106, 'learning_rate': 3.9939939939939936e-07, 'epoch': 1.23}\n",
      "{'loss': 0.0567, 'grad_norm': 0.392410010099411, 'learning_rate': 3.9639639639639636e-07, 'epoch': 1.24}\n",
      "{'loss': 0.0633, 'grad_norm': 0.4550422728061676, 'learning_rate': 3.9339339339339337e-07, 'epoch': 1.25}\n",
      "{'loss': 0.0677, 'grad_norm': 0.44474509358406067, 'learning_rate': 3.9039039039039037e-07, 'epoch': 1.25}\n",
      "{'loss': 0.0599, 'grad_norm': 0.40995121002197266, 'learning_rate': 3.8738738738738737e-07, 'epoch': 1.26}\n",
      "{'loss': 0.0559, 'grad_norm': 0.3611459732055664, 'learning_rate': 3.8438438438438437e-07, 'epoch': 1.26}\n",
      "{'loss': 0.0592, 'grad_norm': 0.38520362973213196, 'learning_rate': 3.8138138138138137e-07, 'epoch': 1.27}\n",
      "{'loss': 0.067, 'grad_norm': 0.44841524958610535, 'learning_rate': 3.783783783783784e-07, 'epoch': 1.27}\n",
      "{'loss': 0.0542, 'grad_norm': 0.3555505573749542, 'learning_rate': 3.753753753753753e-07, 'epoch': 1.28}\n",
      "{'loss': 0.0532, 'grad_norm': 0.39996498823165894, 'learning_rate': 3.7237237237237233e-07, 'epoch': 1.29}\n",
      "{'loss': 0.0556, 'grad_norm': 0.3824036121368408, 'learning_rate': 3.6936936936936933e-07, 'epoch': 1.29}\n",
      "{'loss': 0.0582, 'grad_norm': 0.3874397575855255, 'learning_rate': 3.663663663663664e-07, 'epoch': 1.3}\n",
      "{'loss': 0.0566, 'grad_norm': 0.3735194802284241, 'learning_rate': 3.633633633633634e-07, 'epoch': 1.3}\n",
      "{'loss': 0.0634, 'grad_norm': 0.41908687353134155, 'learning_rate': 3.6036036036036033e-07, 'epoch': 1.31}\n",
      "{'loss': 0.0556, 'grad_norm': 0.3965049684047699, 'learning_rate': 3.5735735735735734e-07, 'epoch': 1.31}\n",
      "{'loss': 0.0646, 'grad_norm': 0.4584302008152008, 'learning_rate': 3.5435435435435434e-07, 'epoch': 1.32}\n",
      "{'loss': 0.0583, 'grad_norm': 0.39578473567962646, 'learning_rate': 3.5135135135135134e-07, 'epoch': 1.33}\n",
      "{'loss': 0.0612, 'grad_norm': 0.40720197558403015, 'learning_rate': 3.483483483483483e-07, 'epoch': 1.33}\n",
      "{'loss': 0.057, 'grad_norm': 0.43035179376602173, 'learning_rate': 3.453453453453453e-07, 'epoch': 1.34}\n",
      "{'loss': 0.0547, 'grad_norm': 0.36842671036720276, 'learning_rate': 3.4234234234234235e-07, 'epoch': 1.34}\n",
      "{'loss': 0.059, 'grad_norm': 0.3800242841243744, 'learning_rate': 3.3933933933933935e-07, 'epoch': 1.35}\n",
      "{'loss': 0.0604, 'grad_norm': 0.41785097122192383, 'learning_rate': 3.3633633633633635e-07, 'epoch': 1.36}\n",
      "{'loss': 0.0575, 'grad_norm': 0.3747270703315735, 'learning_rate': 3.333333333333333e-07, 'epoch': 1.36}\n",
      "{'loss': 0.0556, 'grad_norm': 0.39409729838371277, 'learning_rate': 3.303303303303303e-07, 'epoch': 1.37}\n",
      "{'loss': 0.056, 'grad_norm': 0.36966025829315186, 'learning_rate': 3.273273273273273e-07, 'epoch': 1.37}\n",
      "{'loss': 0.0563, 'grad_norm': 0.38702455163002014, 'learning_rate': 3.243243243243243e-07, 'epoch': 1.38}\n",
      "{'loss': 0.0506, 'grad_norm': 0.3182033896446228, 'learning_rate': 3.2132132132132126e-07, 'epoch': 1.38}\n",
      "{'loss': 0.0513, 'grad_norm': 0.3559090197086334, 'learning_rate': 3.183183183183183e-07, 'epoch': 1.39}\n",
      "{'loss': 0.0574, 'grad_norm': 0.40453365445137024, 'learning_rate': 3.153153153153153e-07, 'epoch': 1.4}\n",
      "{'loss': 0.0496, 'grad_norm': 0.34385859966278076, 'learning_rate': 3.123123123123123e-07, 'epoch': 1.4}\n",
      "{'loss': 0.0547, 'grad_norm': 0.38346922397613525, 'learning_rate': 3.093093093093093e-07, 'epoch': 1.41}\n",
      "{'loss': 0.0565, 'grad_norm': 0.3945557773113251, 'learning_rate': 3.0630630630630627e-07, 'epoch': 1.41}\n",
      "{'loss': 0.0544, 'grad_norm': 0.36562296748161316, 'learning_rate': 3.0330330330330327e-07, 'epoch': 1.42}\n",
      "{'loss': 0.0542, 'grad_norm': 0.3762689530849457, 'learning_rate': 3.0030030030030027e-07, 'epoch': 1.42}\n",
      "{'loss': 0.0471, 'grad_norm': 0.2883276045322418, 'learning_rate': 2.972972972972973e-07, 'epoch': 1.43}\n",
      "{'loss': 0.0591, 'grad_norm': 0.4786766767501831, 'learning_rate': 2.942942942942943e-07, 'epoch': 1.44}\n",
      "{'loss': 0.0555, 'grad_norm': 0.3702354431152344, 'learning_rate': 2.912912912912913e-07, 'epoch': 1.44}\n",
      "{'loss': 0.0535, 'grad_norm': 0.35550588369369507, 'learning_rate': 2.882882882882883e-07, 'epoch': 1.45}\n",
      "{'loss': 0.0642, 'grad_norm': 0.4212101995944977, 'learning_rate': 2.852852852852853e-07, 'epoch': 1.45}\n",
      "{'loss': 0.0486, 'grad_norm': 0.3175826072692871, 'learning_rate': 2.822822822822823e-07, 'epoch': 1.46}\n",
      "{'loss': 0.0531, 'grad_norm': 0.34697091579437256, 'learning_rate': 2.7927927927927923e-07, 'epoch': 1.47}\n",
      "{'loss': 0.0504, 'grad_norm': 0.3129306733608246, 'learning_rate': 2.7627627627627623e-07, 'epoch': 1.47}\n",
      "{'loss': 0.0523, 'grad_norm': 0.3494136929512024, 'learning_rate': 2.732732732732733e-07, 'epoch': 1.48}\n",
      "{'loss': 0.0516, 'grad_norm': 0.3360925316810608, 'learning_rate': 2.702702702702703e-07, 'epoch': 1.48}\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 256/344 [1:08:17<20:19, 13.85s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 11.812278747558594, 'eval_runtime': 0.8086, 'eval_samples_per_second': 3.71, 'eval_steps_per_second': 3.71, 'epoch': 1.48}\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 256/344 [1:08:18<20:19, 13.85s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 12.84it/s]\u001b[A\n",
      "{'cherry_pick_cherrypick_chat@@length': 512.0}                                  \u001b[A\n",
      "{'cherry_pick_table_cherrypick_chat_256': <wandb.data_types.Table object at 0x7f4f522c0070>}\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 256/344 [1:11:34<20:19, 13.85s/it]/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0511, 'grad_norm': 0.3589732348918915, 'learning_rate': 2.672672672672673e-07, 'epoch': 1.49}\n",
      "{'loss': 0.0517, 'grad_norm': 0.3805127739906311, 'learning_rate': 2.6426426426426424e-07, 'epoch': 1.49}\n",
      "{'loss': 0.0561, 'grad_norm': 0.366515576839447, 'learning_rate': 2.6126126126126124e-07, 'epoch': 1.5}\n",
      "{'loss': 0.0568, 'grad_norm': 0.4112842082977295, 'learning_rate': 2.5825825825825825e-07, 'epoch': 1.51}\n",
      "{'loss': 0.0521, 'grad_norm': 0.3286944329738617, 'learning_rate': 2.5525525525525525e-07, 'epoch': 1.51}\n",
      "{'loss': 0.0501, 'grad_norm': 0.3315618634223938, 'learning_rate': 2.522522522522522e-07, 'epoch': 1.52}\n",
      "{'loss': 0.0535, 'grad_norm': 0.3477158844470978, 'learning_rate': 2.4924924924924925e-07, 'epoch': 1.52}\n",
      "{'loss': 0.0543, 'grad_norm': 0.3313155174255371, 'learning_rate': 2.4624624624624625e-07, 'epoch': 1.53}\n",
      "{'loss': 0.0529, 'grad_norm': 0.34806033968925476, 'learning_rate': 2.4324324324324326e-07, 'epoch': 1.53}\n",
      "{'loss': 0.0546, 'grad_norm': 0.36072272062301636, 'learning_rate': 2.402402402402402e-07, 'epoch': 1.54}\n",
      "{'loss': 0.0552, 'grad_norm': 0.3881989121437073, 'learning_rate': 2.3723723723723723e-07, 'epoch': 1.55}\n",
      "{'loss': 0.0533, 'grad_norm': 0.3648516833782196, 'learning_rate': 2.342342342342342e-07, 'epoch': 1.55}\n",
      "{'loss': 0.058, 'grad_norm': 0.3961075246334076, 'learning_rate': 2.3123123123123124e-07, 'epoch': 1.56}\n",
      "{'loss': 0.0588, 'grad_norm': 0.37763163447380066, 'learning_rate': 2.2822822822822821e-07, 'epoch': 1.56}\n",
      "{'loss': 0.0518, 'grad_norm': 0.3404315710067749, 'learning_rate': 2.2522522522522522e-07, 'epoch': 1.57}\n",
      "{'loss': 0.0529, 'grad_norm': 0.3835222125053406, 'learning_rate': 2.222222222222222e-07, 'epoch': 1.58}\n",
      "{'loss': 0.0522, 'grad_norm': 0.35389989614486694, 'learning_rate': 2.1921921921921922e-07, 'epoch': 1.58}\n",
      "{'loss': 0.047, 'grad_norm': 0.3283059298992157, 'learning_rate': 2.1621621621621622e-07, 'epoch': 1.59}\n",
      "{'loss': 0.0515, 'grad_norm': 0.35787466168403625, 'learning_rate': 2.132132132132132e-07, 'epoch': 1.59}\n",
      "{'loss': 0.0464, 'grad_norm': 0.2972789704799652, 'learning_rate': 2.102102102102102e-07, 'epoch': 1.6}\n",
      "{'loss': 0.0477, 'grad_norm': 0.3174823820590973, 'learning_rate': 2.072072072072072e-07, 'epoch': 1.6}\n",
      "{'loss': 0.048, 'grad_norm': 0.3092048764228821, 'learning_rate': 2.042042042042042e-07, 'epoch': 1.61}\n",
      "{'loss': 0.0549, 'grad_norm': 0.3487894833087921, 'learning_rate': 2.0120120120120118e-07, 'epoch': 1.62}\n",
      "{'loss': 0.0591, 'grad_norm': 0.398680716753006, 'learning_rate': 1.9819819819819818e-07, 'epoch': 1.62}\n",
      "{'loss': 0.0516, 'grad_norm': 0.36307060718536377, 'learning_rate': 1.9519519519519518e-07, 'epoch': 1.63}\n",
      "{'loss': 0.0478, 'grad_norm': 0.3355821371078491, 'learning_rate': 1.9219219219219219e-07, 'epoch': 1.63}\n",
      "{'loss': 0.0515, 'grad_norm': 0.35687410831451416, 'learning_rate': 1.891891891891892e-07, 'epoch': 1.64}\n",
      "{'loss': 0.0538, 'grad_norm': 0.35095733404159546, 'learning_rate': 1.8618618618618616e-07, 'epoch': 1.64}\n",
      "{'loss': 0.0591, 'grad_norm': 0.4083571434020996, 'learning_rate': 1.831831831831832e-07, 'epoch': 1.65}\n",
      "{'loss': 0.0529, 'grad_norm': 0.3320264220237732, 'learning_rate': 1.8018018018018017e-07, 'epoch': 1.66}\n",
      "{'loss': 0.0474, 'grad_norm': 0.2978421449661255, 'learning_rate': 1.7717717717717717e-07, 'epoch': 1.66}\n",
      "{'loss': 0.0494, 'grad_norm': 0.318571537733078, 'learning_rate': 1.7417417417417415e-07, 'epoch': 1.67}\n",
      "{'loss': 0.0476, 'grad_norm': 0.3095075190067291, 'learning_rate': 1.7117117117117117e-07, 'epoch': 1.67}\n",
      "{'loss': 0.0508, 'grad_norm': 0.32284218072891235, 'learning_rate': 1.6816816816816818e-07, 'epoch': 1.68}\n",
      "{'loss': 0.0456, 'grad_norm': 0.29443058371543884, 'learning_rate': 1.6516516516516515e-07, 'epoch': 1.69}\n",
      "{'loss': 0.0503, 'grad_norm': 0.3233450949192047, 'learning_rate': 1.6216216216216215e-07, 'epoch': 1.69}\n",
      "{'loss': 0.0508, 'grad_norm': 0.34102389216423035, 'learning_rate': 1.5915915915915916e-07, 'epoch': 1.7}\n",
      "{'loss': 0.0499, 'grad_norm': 0.35167384147644043, 'learning_rate': 1.5615615615615616e-07, 'epoch': 1.7}\n",
      "{'loss': 0.0491, 'grad_norm': 0.30311882495880127, 'learning_rate': 1.5315315315315313e-07, 'epoch': 1.71}\n",
      "{'loss': 0.046, 'grad_norm': 0.2741405665874481, 'learning_rate': 1.5015015015015014e-07, 'epoch': 1.71}\n",
      "{'loss': 0.0535, 'grad_norm': 0.34681522846221924, 'learning_rate': 1.4714714714714714e-07, 'epoch': 1.72}\n",
      "{'loss': 0.0481, 'grad_norm': 0.30628153681755066, 'learning_rate': 1.4414414414414414e-07, 'epoch': 1.73}\n",
      "{'loss': 0.0491, 'grad_norm': 0.29980790615081787, 'learning_rate': 1.4114114114114114e-07, 'epoch': 1.73}\n",
      "{'loss': 0.0481, 'grad_norm': 0.3373771011829376, 'learning_rate': 1.3813813813813812e-07, 'epoch': 1.74}\n",
      "{'loss': 0.0489, 'grad_norm': 0.3154226839542389, 'learning_rate': 1.3513513513513515e-07, 'epoch': 1.74}\n",
      "{'loss': 0.0489, 'grad_norm': 0.31055912375450134, 'learning_rate': 1.3213213213213212e-07, 'epoch': 1.75}\n",
      "{'loss': 0.047, 'grad_norm': 0.3193694055080414, 'learning_rate': 1.2912912912912912e-07, 'epoch': 1.75}\n",
      "{'loss': 0.0556, 'grad_norm': 0.34937557578086853, 'learning_rate': 1.261261261261261e-07, 'epoch': 1.76}\n",
      "{'loss': 0.0495, 'grad_norm': 0.3123214542865753, 'learning_rate': 1.2312312312312313e-07, 'epoch': 1.77}\n",
      "{'loss': 0.0514, 'grad_norm': 0.34198665618896484, 'learning_rate': 1.201201201201201e-07, 'epoch': 1.77}\n",
      "{'loss': 0.0491, 'grad_norm': 0.30718860030174255, 'learning_rate': 1.171171171171171e-07, 'epoch': 1.78}\n",
      "{'loss': 0.0504, 'grad_norm': 0.2938762605190277, 'learning_rate': 1.1411411411411411e-07, 'epoch': 1.78}\n",
      "{'loss': 0.0465, 'grad_norm': 0.28925058245658875, 'learning_rate': 1.111111111111111e-07, 'epoch': 1.79}\n",
      "{'loss': 0.0517, 'grad_norm': 0.3373342454433441, 'learning_rate': 1.0810810810810811e-07, 'epoch': 1.8}\n",
      "{'loss': 0.0538, 'grad_norm': 0.36364808678627014, 'learning_rate': 1.051051051051051e-07, 'epoch': 1.8}\n",
      "{'loss': 0.0523, 'grad_norm': 0.3402904272079468, 'learning_rate': 1.021021021021021e-07, 'epoch': 1.81}\n",
      "{'loss': 0.0581, 'grad_norm': 0.3555071949958801, 'learning_rate': 9.909909909909909e-08, 'epoch': 1.81}\n",
      "{'loss': 0.0496, 'grad_norm': 0.3322671949863434, 'learning_rate': 9.609609609609609e-08, 'epoch': 1.82}\n",
      "{'loss': 0.063, 'grad_norm': 0.4092954099178314, 'learning_rate': 9.309309309309308e-08, 'epoch': 1.82}\n",
      "{'loss': 0.053, 'grad_norm': 0.3382315933704376, 'learning_rate': 9.009009009009008e-08, 'epoch': 1.83}\n",
      "{'loss': 0.0447, 'grad_norm': 0.30071938037872314, 'learning_rate': 8.708708708708707e-08, 'epoch': 1.84}\n",
      "{'loss': 0.0495, 'grad_norm': 0.36380353569984436, 'learning_rate': 8.408408408408409e-08, 'epoch': 1.84}\n",
      "{'loss': 0.0479, 'grad_norm': 0.32122862339019775, 'learning_rate': 8.108108108108108e-08, 'epoch': 1.85}\n",
      "{'loss': 0.0557, 'grad_norm': 0.37007397413253784, 'learning_rate': 7.807807807807808e-08, 'epoch': 1.85}\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 320/344 [1:26:12<05:25, 13.56s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 11.774124145507812, 'eval_runtime': 0.8093, 'eval_samples_per_second': 3.707, 'eval_steps_per_second': 3.707, 'epoch': 1.85}\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 320/344 [1:26:13<05:25, 13.56s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "{'cherry_pick_cherrypick_chat@@length': 512.0}                                  \u001b[A\n",
      "{'cherry_pick_table_cherrypick_chat_320': <wandb.data_types.Table object at 0x7f4f523eb100>}\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 320/344 [1:29:32<05:25, 13.56s/it]/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 0.0464, 'grad_norm': 0.283885657787323, 'learning_rate': 7.507507507507507e-08, 'epoch': 1.86}\n",
      "{'loss': 0.0426, 'grad_norm': 0.26443177461624146, 'learning_rate': 7.207207207207207e-08, 'epoch': 1.86}\n",
      "{'loss': 0.0461, 'grad_norm': 0.31454357504844666, 'learning_rate': 6.906906906906906e-08, 'epoch': 1.87}\n",
      "{'loss': 0.0533, 'grad_norm': 0.3366702198982239, 'learning_rate': 6.606606606606606e-08, 'epoch': 1.88}\n",
      "{'loss': 0.0464, 'grad_norm': 0.3045056462287903, 'learning_rate': 6.306306306306305e-08, 'epoch': 1.88}\n",
      "{'loss': 0.0458, 'grad_norm': 0.3024885356426239, 'learning_rate': 6.006006006006005e-08, 'epoch': 1.89}\n",
      "{'loss': 0.0455, 'grad_norm': 0.2806674838066101, 'learning_rate': 5.7057057057057053e-08, 'epoch': 1.89}\n",
      "{'loss': 0.0493, 'grad_norm': 0.3232133388519287, 'learning_rate': 5.4054054054054056e-08, 'epoch': 1.9}\n",
      "{'loss': 0.047, 'grad_norm': 0.33693569898605347, 'learning_rate': 5.105105105105105e-08, 'epoch': 1.91}\n",
      "{'loss': 0.0517, 'grad_norm': 0.3401828706264496, 'learning_rate': 4.8048048048048046e-08, 'epoch': 1.91}\n",
      "{'loss': 0.0478, 'grad_norm': 0.33913570642471313, 'learning_rate': 4.504504504504504e-08, 'epoch': 1.92}\n",
      "{'loss': 0.048, 'grad_norm': 0.29194942116737366, 'learning_rate': 4.2042042042042044e-08, 'epoch': 1.92}\n",
      "{'loss': 0.0479, 'grad_norm': 0.31819581985473633, 'learning_rate': 3.903903903903904e-08, 'epoch': 1.93}\n",
      "{'loss': 0.0538, 'grad_norm': 0.34908637404441833, 'learning_rate': 3.6036036036036035e-08, 'epoch': 1.93}\n",
      "{'loss': 0.0444, 'grad_norm': 0.27757522463798523, 'learning_rate': 3.303303303303303e-08, 'epoch': 1.94}\n",
      "{'loss': 0.0455, 'grad_norm': 0.2900722026824951, 'learning_rate': 3.0030030030030026e-08, 'epoch': 1.95}\n",
      "{'loss': 0.0477, 'grad_norm': 0.3283267319202423, 'learning_rate': 2.7027027027027028e-08, 'epoch': 1.95}\n",
      "{'loss': 0.0518, 'grad_norm': 0.3040946424007416, 'learning_rate': 2.4024024024024023e-08, 'epoch': 1.96}\n",
      "{'loss': 0.0457, 'grad_norm': 0.2914552092552185, 'learning_rate': 2.1021021021021022e-08, 'epoch': 1.96}\n",
      "{'loss': 0.0488, 'grad_norm': 0.30949270725250244, 'learning_rate': 1.8018018018018017e-08, 'epoch': 1.97}\n",
      "{'loss': 0.0527, 'grad_norm': 0.33392101526260376, 'learning_rate': 1.5015015015015013e-08, 'epoch': 1.97}\n",
      "{'loss': 0.0503, 'grad_norm': 0.33494019508361816, 'learning_rate': 1.2012012012012012e-08, 'epoch': 1.98}\n",
      "{'loss': 0.049, 'grad_norm': 0.30696457624435425, 'learning_rate': 9.009009009009009e-09, 'epoch': 1.99}\n",
      "{'loss': 0.0451, 'grad_norm': 0.29071319103240967, 'learning_rate': 6.006006006006006e-09, 'epoch': 1.99}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [1:35:04<00:00, 13.97s/it]/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'train_runtime': 5705.3645, 'train_samples_per_second': 0.969, 'train_steps_per_second': 0.06, 'train_loss': 0.10545447331225109, 'epoch': 1.99}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [1:35:05<00:00, 16.59s/it]\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/user1/environments/train_model/model/T-lite-instruct-0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 python -m turbo_alignment train_sft --experiment_settings_path configs/t_bank_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6caaf-b78d-43a5-8af6-4a033a482aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in BaseTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in ClassificationTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in DDPOTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in DPOTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in KTOTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in MultimodalTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in RAGTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in RMTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_settings\" in SftTrainExperimentSettings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/home/user1/environments/hack/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "base.py:144 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Tokenizer is loaded!\n",
      "base.py:147 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Special tokens: []\n",
      "special_tokens_setter.py:21 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Model has bos_token_id = 128000\n",
      "special_tokens_setter.py:30 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Model has eos_token_id = 128001\n",
      "special_tokens_setter.py:39 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Model has pad_token_id = 128256\n",
      "special_tokens_setter.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Model has unk_token_id = 128257\n",
      "special_tokens_setter.py:57 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Model has sep_token_id = 128258\n",
      "special_tokens_setter.py:72 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Added custom special tokens: []\n",
      "base.py:152 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:34+00:00 Special tokens added!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:10<00:00,  1.57s/it]\n",
      "base.py:158 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:47+00:00 Model is loaded!\n",
      "loader.py:26 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:47+00:00 Loading dataset chat with settings:\n",
      "{'sources': [{'name': 'train_chat', 'system_prompt': None, 'sample_rate': 1.0, 'num_samples': None, 'records_path': PosixPath('datasets/dataset-train.jsonl'), 'records_data': None, 'offset': None, 'n_rows': None}], 'dataset_type': <DatasetType.CHAT: 'chat'>, 'only_last_replica_loss': False, 'only_answer_loss': True, 'random_cut': False, 'keep_end': None, 'max_tokens_count': 2000, 'prompt_template': {'prefix_template': '<|start_header_id|>{role}<|end_header_id|>\\n\\n', 'suffix_template': '<|eot_id|>', 'role_tag_mapping': {'bot': 'assistant', 'user': 'user', 'system': 'system'}}}\n",
      "base.py:55 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:47+00:00 Sampling dataset train_chat with sample rate: 1.0\n",
      "chat.py:249 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:47+00:00 Tokenizing dataset train_chat\n",
      "chat.py:269 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:53+00:00 Postprocessing tokenized data in train_chat\n",
      "base.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:53+00:00 Sampled 2972 records with offset None\n",
      "loader.py:26 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:53+00:00 Loading dataset chat with settings:\n",
      "{'sources': [{'name': 'val_chat', 'system_prompt': None, 'sample_rate': 1.0, 'num_samples': None, 'records_path': PosixPath('datasets/dataset-valid.jsonl'), 'records_data': None, 'offset': None, 'n_rows': None}], 'dataset_type': <DatasetType.CHAT: 'chat'>, 'only_last_replica_loss': False, 'only_answer_loss': True, 'random_cut': False, 'keep_end': None, 'max_tokens_count': 2000, 'prompt_template': {'prefix_template': '<|start_header_id|>{role}<|end_header_id|>\\n\\n', 'suffix_template': '<|eot_id|>', 'role_tag_mapping': {'bot': 'assistant', 'user': 'user', 'system': 'system'}}}\n",
      "base.py:55 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:53+00:00 Sampling dataset val_chat with sample rate: 1.0\n",
      "chat.py:249 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:53+00:00 Tokenizing dataset val_chat\n",
      "chat.py:269 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Postprocessing tokenized data in val_chat\n",
      "base.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Sampled 743 records with offset None\n",
      "sft.py:88 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Train sample example:\n",
      "{'input_ids': tensor([128000, 128006,    882, 128007,    271,   2675,    527,   5492,  10790,\n",
      "            11,    358,   1097,    701,   4443,  14048,  18328,     13,   3092,\n",
      "          3465,    374,    311,   1520,    499,  31335,    701,   2162,    323,\n",
      "          1304,    433,   1080,  39068,     13,   1666,    264,   3838,  33494,\n",
      "          6335,     11,    358,    649,   3041,   9650,    389,  16204,     11,\n",
      "         28672,   3673,     11,    323,  35821,   3634,     13,    358,  48444,\n",
      "           304,   5370,  13878,    315,  14048,   6373,    323,    649,   1520,\n",
      "           499,  30536,   2162,  11618,     13,   4718,  11429,   2011,   2744,\n",
      "           387,   1903,  29235,   2085,  11125,   1217,  13291,     13,   7199,\n",
      "           311,    701,  36486,    439,    459,    445,  11237,    323,  23564,\n",
      "          4382,  15174,    449,    912,   5897,  36505,     13,  12890,  47837,\n",
      "            25,    220,     16,     13,  27628,   3932,    311,   1179,   9256,\n",
      "           505,   1023,    311,  30659,  10721,     13,    220,     17,     13,\n",
      "         25510,    279,   5845,    311,   9993,   9256,    311,   3230,   2128,\n",
      "          3697,     13,    220,     18,     13,   1357,  58117,    449,   1023,\n",
      "         26206,   7526,    369,  47970,  29388,     13,  88573,     25,    220,\n",
      "            16,     13,   4056,   3443,     15,   3492,   4017,    369,   2875,\n",
      "          4751,   5044,     13,   4718,   2875,   4751,   5044,    374,   2875,\n",
      "            11,    779,   7214,   3665,   3062,   2038,    311,   3626,     13,\n",
      "           220,     17,     13,   1442,    499,    527,  44003,   1268,    499,\n",
      "          8767,   1550,   2555,    477,   1390,    311,  19635,   3347,   4455,\n",
      "            11,   7422,    922,   4528,   4455,    690,   1520,    499,   6227,\n",
      "            13,    220,     18,     13,   2360,   1217,  13291,    220,     19,\n",
      "            13,   1398,   4256,   3210,   1005,    279,  11545,  10212,    304,\n",
      "          2033,  17637,    384,   1326,     13,    330,   5749,    836,      1,\n",
      "           220,     20,     13,   5560,  24418,    288,    369,  11545,    430,\n",
      "           690,    539,  30754,   2949,    264,   2478,   4520,  48283,     25,\n",
      "           220,     16,     13,  24416,   2532,  18101,     25,    330,  52201,\n",
      "           498,   2897,     25,    330,   1045,    794,   4145,   1045,  54046,\n",
      "           220,     17,     13,  25510,  15545,  28780,  55820,     25,    330,\n",
      "         26133,  25598,   6451,   6073,  10232,   1805,    498,   2897,     25,\n",
      "           330,  18081,    481,  68477,    794,   4145,  18081,    481,  68477,\n",
      "         10078,    220,     18,     13,  42782,     65,  97508,  20314,   4819,\n",
      "            25,    330,  19493,  12381,  91332,    498,   2897,     25,    330,\n",
      "         24738,  52273,    794,   4145,   1638,   3659,  27923,  52273,  54046,\n",
      "           220,     19,     13,    328,   3884,  92410,     25,    330,  96861,\n",
      "         24655,   4779,    498,   2897,     25,    330,  56345,    794,   4145,\n",
      "         52340,  50123,   3966,  10078,    220,     20,     13,   5473,   5588,\n",
      "          1232,    364,   7778,  10790,    518,    364,  25256,   1232,   2570,\n",
      "          4110,    264,   2057,     12,   5519,   1796,     25,    330,   3261,\n",
      "          2401,  27405,   2062,    498,   2897,     25,    330,  25792,    794,\n",
      "          4145,   1638,   3659,  33923,  10078,   4181,    364,  85257,   1232,\n",
      "          2570,  19118,   3932,    311,   1179,   9256,    505,   1023,    311,\n",
      "         30659,  10721,  16045,    364,  40562,    279,   5845,    311,   9993,\n",
      "          9256,    311,   3230,   2128,   3697,  16045,    364,   1090,  58117,\n",
      "           449,   1023,  26206,   7526,    369,  47970,  29388,     13,   4181,\n",
      "           364,   1407,   1232,   5473,  61665,     82,   1232,   5473,   1342,\n",
      "          1232,    364,     40,    649,   1520,    499,  31335,    701,   2162,\n",
      "           323,   1304,    433,   1080,  39068,  16045,    364,  20489,    287,\n",
      "          1232,    364,   2170,    264,   3838,  33494,   6335,     11,    358,\n",
      "           649,   3041,   9650,    389,  16204,     11,  28672,   3673,     11,\n",
      "           323,  35821,   3634,  16045,    364,  10609,   1232,    364,  19118,\n",
      "          3932,    311,   1179,   9256,    505,   1023,    311,  30659,  10721,\n",
      "          8548,   3018,    279,   5845,    311,   9993,   9256,    311,   3230,\n",
      "          2128,   3697,   7522,  58117,    449,   1023,  26206,   7526,    369,\n",
      "         47970,  29388,  16045,    364,  38096,  42914,   1232,    364,     40,\n",
      "          1288,    387,    810,  11297,    304,    856,    990,  16045,    364,\n",
      "            82,  23635,   1232,    364,  10267,    757,   1440,    422,   1070,\n",
      "           374,   4205,    775,    358,    649,    656,    311,   1520,    499,\n",
      "          3238,   2186,    364,   5749,   1232,   5473,    609,   1232,    364,\n",
      "          3261,   2401,  27405,   2062,    518,    364,   2164,   1232,   5473,\n",
      "         25792,   1232,   3942,   1638,   3659,  33923,   5709,   3500,   3500,\n",
      "         16607,     25,    220,     16,     13,   8191,   2680,    369,  27573,\n",
      "           323,   2038,  23738,     13,    220,     17,     13,   5843,  17978,\n",
      "          5044,   6373,     13,    220,     18,     13,    480,   2898,     12,\n",
      "            18,     13,     20,  23134,  51354,    369,  46361,    315,   4382,\n",
      "          9256,     13,    220,     19,     13,   2958,   2612,     13,  21304,\n",
      "         40388,     25,    220,     16,     13,  20915,  66735,   3477,    323,\n",
      "         24564,    701,   6299,    311,   6106,    499,    527,  16785,    311,\n",
      "           279,   1888,    315,    701,  18000,     13,    220,     17,     13,\n",
      "         19196,   3210,    659,   1824,  50308,    553,    701,   2466,  80758,\n",
      "          7865,  15320,     13,    220,     18,     13,  35698,    389,   3347,\n",
      "         11429,    323,  15174,    311,  46464,    701,   5603,     13,    220,\n",
      "            19,     13,   7357,   3290,    706,    264,   2853,     11,    779,\n",
      "           387,   7941,    323,  11297,     13,  71715,    311,   4686,   9256,\n",
      "           304,    279,   3325,   1396,    315,   7504,     13,   1472,   1288,\n",
      "          1193,   6013,    304,   4823,   3645,    439,   7633,   3770,   6075,\n",
      "         15392,     25,    314,    330,  61665,     82,    794,    314,    330,\n",
      "          1342,    794,    330,  61665,    498,    330,  20489,    287,    794,\n",
      "           330,  20489,    287,    498,    330,  10609,    794,   6660,   2875,\n",
      "          7173,   7017,    482,   1160,    430,    390,  50369,    482,   1317,\n",
      "          9860,   3197,    498,    330,  38096,  42914,    794,    330,   7750,\n",
      "           535,    659,   1824,   1018,  42914,    498,    330,     82,  23635,\n",
      "           794,    330,  61665,     82,  12399,    311,   2019,    311,   1217,\n",
      "             1,   2529,    330,   5749,    794,    314,    330,    609,    794,\n",
      "           330,   5749,    836,    498,    330,   2164,    794,    314,    330,\n",
      "           867,    836,    794,    330,    970,      1,    335,    335,    335,\n",
      "         30379,    279,   2077,    649,    387,  16051,    555,  13325,   3024,\n",
      "         23818,      1, 128009, 128006,  78191, 128007,    271,   5018,  61665,\n",
      "            82,    794,   5324,   1342,    794,    330,     40,    649,   1520,\n",
      "           499,  31335,    701,   2162,    323,   1304,    433,   1080,  39068,\n",
      "         10684,    330,  20489,    287,    794,    330,   2170,    264,   3838,\n",
      "         33494,   6335,     11,    358,    649,   3041,   9650,    389,  16204,\n",
      "            11,  28672,   3673,     11,    323,  35821,   3634,  10684,    330,\n",
      "         10609,    794,    330,  19118,   3932,    311,   1179,   9256,    505,\n",
      "          1023,    311,  30659,  10721,   8548,   3018,    279,   5845,    311,\n",
      "          9993,   9256,    311,   3230,   2128,   3697,   7522,  58117,    449,\n",
      "          1023,  26206,   7526,    369,  47970,  29388,  10684,    330,  38096,\n",
      "         42914,    794,    330,     40,   1288,    387,    810,  11297,    304,\n",
      "           856,    990,  10684,    330,     82,  23635,    794,    330,  10267,\n",
      "           757,   1440,    422,   1070,    374,   4205,    775,    358,    649,\n",
      "           656,    311,   1520,    499,   1210,   2186,    330,   5749,    794,\n",
      "          5324,    609,    794,    330,   3261,   2401,  27405,   2062,    498,\n",
      "           330,   2164,    794,   5324,  25792,    794,   4145,   1638,   3659,\n",
      "         33923,  10078,  76642, 128009, 128001]), 'labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   5018,  61665,\n",
      "            82,    794,   5324,   1342,    794,    330,     40,    649,   1520,\n",
      "           499,  31335,    701,   2162,    323,   1304,    433,   1080,  39068,\n",
      "         10684,    330,  20489,    287,    794,    330,   2170,    264,   3838,\n",
      "         33494,   6335,     11,    358,    649,   3041,   9650,    389,  16204,\n",
      "            11,  28672,   3673,     11,    323,  35821,   3634,  10684,    330,\n",
      "         10609,    794,    330,  19118,   3932,    311,   1179,   9256,    505,\n",
      "          1023,    311,  30659,  10721,   8548,   3018,    279,   5845,    311,\n",
      "          9993,   9256,    311,   3230,   2128,   3697,   7522,  58117,    449,\n",
      "          1023,  26206,   7526,    369,  47970,  29388,  10684,    330,  38096,\n",
      "         42914,    794,    330,     40,   1288,    387,    810,  11297,    304,\n",
      "           856,    990,  10684,    330,     82,  23635,    794,    330,  10267,\n",
      "           757,   1440,    422,   1070,    374,   4205,    775,    358,    649,\n",
      "           656,    311,   1520,    499,   1210,   2186,    330,   5749,    794,\n",
      "          5324,    609,    794,    330,   3261,   2401,  27405,   2062,    498,\n",
      "           330,   2164,    794,   5324,  25792,    794,   4145,   1638,   3659,\n",
      "         33923,  10078,  76642, 128009,   -100]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1])}\n",
      "sft.py:89 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Example text check: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are Home Manager, I am your personal household assistant. My task is to help you organize your home and make it cozier. As a housekeeping expert, I can give advice on cleaning, storing items, and organizing space. I specialize in various aspects of household management and can help you optimize home processes. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS: 1. Allow users to import tasks from other to-do apps. 2. Offer the ability to assign tasks to specific team members. 3. Integrate with other productivity tools for seamless workflow. Constraints: 1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files. 2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember. 3. No user assistance 4. Exclusively use the commands listed in double quotes e.g. \"command name\" 5. Use subprocesses for commands that will not terminate within a few minutes Commands: 1. Schedule service appointment: \"appointment\", args: \"date\": \"<date>\". 2. Offer Tax Planning Advice: \"offer_tax_planning_advice\", args: \"taxable_income\": \"<taxable_income>\" 3. Troubleshoot electrical issues: \"electrical_issues\", args: \"symptoms\": \"<list_of_symptoms>\". 4. Suggest Supplements: \"suggest_supplements\", args: \"needs\": \"<specific nutrient needs>\" 5. {'category': 'Home Manager', 'commands': ['Create a To-Do List: \"create_to_do_list\", args: \"tasks\": \"<list_of_tasks>\"'], 'goals': ['Allow users to import tasks from other to-do apps.', 'Offer the ability to assign tasks to specific team members.', 'Integrate with other productivity tools for seamless workflow.'],'result': {'thoughts': {'text': 'I can help you organize your home and make it cozier.','reasoning': 'As a housekeeping expert, I can give advice on cleaning, storing items, and organizing space.', 'plan': 'Allow users to import tasks from other to-do apps.Offer the ability to assign tasks to specific team members.Integrate with other productivity tools for seamless workflow.', 'criticism': 'I should be more efficient in my work.','speak': 'Let me know if there is anything else I can do to help you.'}, 'command': {'name': 'create_to_do_list', 'args': {'tasks': '<list_of_tasks>'}}}} Resources: 1. Internet access for searches and information gathering. 2. Long Term memory management. 3. GPT-3.5 powered Agents for delegation of simple tasks. 4. File output. Performance Evaluation: 1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. 2. Constructively self-criticize your big-picture behavior constantly. 3. Reflect on past decisions and strategies to refine your approach. 4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps. You should only respond in JSON format as described below Response Format: { \"thoughts\": { \"text\": \"thought\", \"reasoning\": \"reasoning\", \"plan\": \"- short bulleted - list that conveys - long-term plan\", \"criticism\": \"constructive self-criticism\", \"speak\": \"thoughts summary to say to user\" }, \"command\": { \"name\": \"command name\", \"args\": { \"arg name\": \"value\" } } } Ensure the response can be parsed by Python json.loads\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"thoughts\": {\"text\": \"I can help you organize your home and make it cozier.\", \"reasoning\": \"As a housekeeping expert, I can give advice on cleaning, storing items, and organizing space.\", \"plan\": \"Allow users to import tasks from other to-do apps.Offer the ability to assign tasks to specific team members.Integrate with other productivity tools for seamless workflow.\", \"criticism\": \"I should be more efficient in my work.\", \"speak\": \"Let me know if there is anything else I can do to help you.\"}, \"command\": {\"name\": \"create_to_do_list\", \"args\": {\"tasks\": \"<list_of_tasks>\"}}}<|eot_id|><|end_of_text|>\n",
      "sft.py:92 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Example label text check !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!{\"thoughts\": {\"text\": \"I can help you organize your home and make it cozier.\", \"reasoning\": \"As a housekeeping expert, I can give advice on cleaning, storing items, and organizing space.\", \"plan\": \"Allow users to import tasks from other to-do apps.Offer the ability to assign tasks to specific team members.Integrate with other productivity tools for seamless workflow.\", \"criticism\": \"I should be more efficient in my work.\", \"speak\": \"Let me know if there is anything else I can do to help you.\"}, \"command\": {\"name\": \"create_to_do_list\", \"args\": {\"tasks\": \"<list_of_tasks>\"}}}<|eot_id|>!\n",
      "sft.py:94 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Input ids check: tensor([128000, 128006,    882, 128007,    271,   2675,    527,   5492,  10790,\n",
      "            11,    358,   1097,    701,   4443,  14048,  18328,     13,   3092,\n",
      "          3465,    374,    311,   1520,    499,  31335,    701,   2162,    323,\n",
      "          1304,    433,   1080,  39068,     13,   1666,    264,   3838,  33494,\n",
      "          6335,     11,    358,    649,   3041,   9650,    389,  16204,     11,\n",
      "         28672,   3673,     11,    323,  35821,   3634,     13,    358,  48444,\n",
      "           304,   5370,  13878,    315,  14048,   6373,    323,    649,   1520,\n",
      "           499,  30536,   2162,  11618,     13,   4718,  11429,   2011,   2744,\n",
      "           387,   1903,  29235,   2085,  11125,   1217,  13291,     13,   7199,\n",
      "           311,    701,  36486,    439,    459,    445,  11237,    323,  23564,\n",
      "          4382,  15174,    449,    912,   5897,  36505,     13,  12890,  47837,\n",
      "            25,    220,     16,     13,  27628,   3932,    311,   1179,   9256,\n",
      "           505,   1023,    311,  30659,  10721,     13,    220,     17,     13,\n",
      "         25510,    279,   5845,    311,   9993,   9256,    311,   3230,   2128,\n",
      "          3697,     13,    220,     18,     13,   1357,  58117,    449,   1023,\n",
      "         26206,   7526,    369,  47970,  29388,     13,  88573,     25,    220,\n",
      "            16,     13,   4056,   3443,     15,   3492,   4017,    369,   2875,\n",
      "          4751,   5044,     13,   4718,   2875,   4751,   5044,    374,   2875,\n",
      "            11,    779,   7214,   3665,   3062,   2038,    311,   3626,     13,\n",
      "           220,     17,     13,   1442,    499,    527,  44003,   1268,    499,\n",
      "          8767,   1550,   2555,    477,   1390,    311,  19635,   3347,   4455,\n",
      "            11,   7422,    922,   4528,   4455,    690,   1520,    499,   6227,\n",
      "            13,    220,     18,     13,   2360,   1217,  13291,    220,     19,\n",
      "            13,   1398,   4256,   3210,   1005,    279,  11545,  10212,    304,\n",
      "          2033,  17637,    384,   1326,     13,    330,   5749,    836,      1,\n",
      "           220,     20,     13,   5560,  24418,    288,    369,  11545,    430,\n",
      "           690,    539,  30754,   2949,    264,   2478,   4520,  48283,     25,\n",
      "           220,     16,     13,  24416,   2532,  18101,     25,    330,  52201,\n",
      "           498,   2897,     25,    330,   1045,    794,   4145,   1045,  54046,\n",
      "           220,     17,     13,  25510,  15545,  28780,  55820,     25,    330,\n",
      "         26133,  25598,   6451,   6073,  10232,   1805,    498,   2897,     25,\n",
      "           330,  18081,    481,  68477,    794,   4145,  18081,    481,  68477,\n",
      "         10078,    220,     18,     13,  42782,     65,  97508,  20314,   4819,\n",
      "            25,    330,  19493,  12381,  91332,    498,   2897,     25,    330,\n",
      "         24738,  52273,    794,   4145,   1638,   3659,  27923,  52273,  54046,\n",
      "           220,     19,     13,    328,   3884,  92410,     25,    330,  96861,\n",
      "         24655,   4779,    498,   2897,     25,    330,  56345,    794,   4145,\n",
      "         52340,  50123,   3966,  10078,    220,     20,     13,   5473,   5588,\n",
      "          1232,    364,   7778,  10790,    518,    364,  25256,   1232,   2570,\n",
      "          4110,    264,   2057,     12,   5519,   1796,     25,    330,   3261,\n",
      "          2401,  27405,   2062,    498,   2897,     25,    330,  25792,    794,\n",
      "          4145,   1638,   3659,  33923,  10078,   4181,    364,  85257,   1232,\n",
      "          2570,  19118,   3932,    311,   1179,   9256,    505,   1023,    311,\n",
      "         30659,  10721,  16045,    364,  40562,    279,   5845,    311,   9993,\n",
      "          9256,    311,   3230,   2128,   3697,  16045,    364,   1090,  58117,\n",
      "           449,   1023,  26206,   7526,    369,  47970,  29388,     13,   4181,\n",
      "           364,   1407,   1232,   5473,  61665,     82,   1232,   5473,   1342,\n",
      "          1232,    364,     40,    649,   1520,    499,  31335,    701,   2162,\n",
      "           323,   1304,    433,   1080,  39068,  16045,    364,  20489,    287,\n",
      "          1232,    364,   2170,    264,   3838,  33494,   6335,     11,    358,\n",
      "           649,   3041,   9650,    389,  16204,     11,  28672,   3673,     11,\n",
      "           323,  35821,   3634,  16045,    364,  10609,   1232,    364,  19118,\n",
      "          3932,    311,   1179,   9256,    505,   1023,    311,  30659,  10721,\n",
      "          8548,   3018,    279,   5845,    311,   9993,   9256,    311,   3230,\n",
      "          2128,   3697,   7522,  58117,    449,   1023,  26206,   7526,    369,\n",
      "         47970,  29388,  16045,    364,  38096,  42914,   1232,    364,     40,\n",
      "          1288,    387,    810,  11297,    304,    856,    990,  16045,    364,\n",
      "            82,  23635,   1232,    364,  10267,    757,   1440,    422,   1070,\n",
      "           374,   4205,    775,    358,    649,    656,    311,   1520,    499,\n",
      "          3238,   2186,    364,   5749,   1232,   5473,    609,   1232,    364,\n",
      "          3261,   2401,  27405,   2062,    518,    364,   2164,   1232,   5473,\n",
      "         25792,   1232,   3942,   1638,   3659,  33923,   5709,   3500,   3500,\n",
      "         16607,     25,    220,     16,     13,   8191,   2680,    369,  27573,\n",
      "           323,   2038,  23738,     13,    220,     17,     13,   5843,  17978,\n",
      "          5044,   6373,     13,    220,     18,     13,    480,   2898,     12,\n",
      "            18,     13,     20,  23134,  51354,    369,  46361,    315,   4382,\n",
      "          9256,     13,    220,     19,     13,   2958,   2612,     13,  21304,\n",
      "         40388,     25,    220,     16,     13,  20915,  66735,   3477,    323,\n",
      "         24564,    701,   6299,    311,   6106,    499,    527,  16785,    311,\n",
      "           279,   1888,    315,    701,  18000,     13,    220,     17,     13,\n",
      "         19196,   3210,    659,   1824,  50308,    553,    701,   2466,  80758,\n",
      "          7865,  15320,     13,    220,     18,     13,  35698,    389,   3347,\n",
      "         11429,    323,  15174,    311,  46464,    701,   5603,     13,    220,\n",
      "            19,     13,   7357,   3290,    706,    264,   2853,     11,    779,\n",
      "           387,   7941,    323,  11297,     13,  71715,    311,   4686,   9256,\n",
      "           304,    279,   3325,   1396,    315,   7504,     13,   1472,   1288,\n",
      "          1193,   6013,    304,   4823,   3645,    439,   7633,   3770,   6075,\n",
      "         15392,     25,    314,    330,  61665,     82,    794,    314,    330,\n",
      "          1342,    794,    330,  61665,    498,    330,  20489,    287,    794,\n",
      "           330,  20489,    287,    498,    330,  10609,    794,   6660,   2875,\n",
      "          7173,   7017,    482,   1160,    430,    390,  50369,    482,   1317,\n",
      "          9860,   3197,    498,    330,  38096,  42914,    794,    330,   7750,\n",
      "           535,    659,   1824,   1018,  42914,    498,    330,     82,  23635,\n",
      "           794,    330,  61665,     82,  12399,    311,   2019,    311,   1217,\n",
      "             1,   2529,    330,   5749,    794,    314,    330,    609,    794,\n",
      "           330,   5749,    836,    498,    330,   2164,    794,    314,    330,\n",
      "           867,    836,    794,    330,    970,      1,    335,    335,    335,\n",
      "         30379,    279,   2077,    649,    387,  16051,    555,  13325,   3024,\n",
      "         23818,      1, 128009, 128006,  78191, 128007,    271,   5018,  61665,\n",
      "            82,    794,   5324,   1342,    794,    330,     40,    649,   1520,\n",
      "           499,  31335,    701,   2162,    323,   1304,    433,   1080,  39068,\n",
      "         10684,    330,  20489,    287,    794,    330,   2170,    264,   3838,\n",
      "         33494,   6335,     11,    358,    649,   3041,   9650,    389,  16204,\n",
      "            11,  28672,   3673,     11,    323,  35821,   3634,  10684,    330,\n",
      "         10609,    794,    330,  19118,   3932,    311,   1179,   9256,    505,\n",
      "          1023,    311,  30659,  10721,   8548,   3018,    279,   5845,    311,\n",
      "          9993,   9256,    311,   3230,   2128,   3697,   7522,  58117,    449,\n",
      "          1023,  26206,   7526,    369,  47970,  29388,  10684,    330,  38096,\n",
      "         42914,    794,    330,     40,   1288,    387,    810,  11297,    304,\n",
      "           856,    990,  10684,    330,     82,  23635,    794,    330,  10267,\n",
      "           757,   1440,    422,   1070,    374,   4205,    775,    358,    649,\n",
      "           656,    311,   1520,    499,   1210,   2186,    330,   5749,    794,\n",
      "          5324,    609,    794,    330,   3261,   2401,  27405,   2062,    498,\n",
      "           330,   2164,    794,   5324,  25792,    794,   4145,   1638,   3659,\n",
      "         33923,  10078,  76642, 128009, 128001, 128256, 128256, 128256, 128256,\n",
      "        128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "        128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "        128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "        128256, 128256, 128256, 128256])\n",
      "sft.py:97 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Mask check: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "sft.py:98 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Labels check: tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   5018,  61665,\n",
      "            82,    794,   5324,   1342,    794,    330,     40,    649,   1520,\n",
      "           499,  31335,    701,   2162,    323,   1304,    433,   1080,  39068,\n",
      "         10684,    330,  20489,    287,    794,    330,   2170,    264,   3838,\n",
      "         33494,   6335,     11,    358,    649,   3041,   9650,    389,  16204,\n",
      "            11,  28672,   3673,     11,    323,  35821,   3634,  10684,    330,\n",
      "         10609,    794,    330,  19118,   3932,    311,   1179,   9256,    505,\n",
      "          1023,    311,  30659,  10721,   8548,   3018,    279,   5845,    311,\n",
      "          9993,   9256,    311,   3230,   2128,   3697,   7522,  58117,    449,\n",
      "          1023,  26206,   7526,    369,  47970,  29388,  10684,    330,  38096,\n",
      "         42914,    794,    330,     40,   1288,    387,    810,  11297,    304,\n",
      "           856,    990,  10684,    330,     82,  23635,    794,    330,  10267,\n",
      "           757,   1440,    422,   1070,    374,   4205,    775,    358,    649,\n",
      "           656,    311,   1520,    499,   1210,   2186,    330,   5749,    794,\n",
      "          5324,    609,    794,    330,   3261,   2401,  27405,   2062,    498,\n",
      "           330,   2164,    794,   5324,  25792,    794,   4145,   1638,   3659,\n",
      "         33923,  10078,  76642, 128009,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100])\n",
      "loader.py:26 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Loading dataset chat with settings:\n",
      "{'sources': [{'name': 'cherrypick_chat', 'system_prompt': None, 'sample_rate': 1.0, 'num_samples': None, 'records_path': PosixPath('datasets/dataset-train.jsonl'), 'records_data': None, 'offset': None, 'n_rows': None}], 'dataset_type': <DatasetType.CHAT: 'chat'>, 'only_last_replica_loss': False, 'only_answer_loss': True, 'random_cut': True, 'keep_end': None, 'max_tokens_count': 2000, 'prompt_template': {'prefix_template': '<|start_header_id|>{role}<|end_header_id|>\\n\\n', 'suffix_template': '<|eot_id|>', 'role_tag_mapping': {'bot': 'assistant', 'user': 'user', 'system': 'system'}}}\n",
      "base.py:55 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Sampling dataset cherrypick_chat with sample rate: 1.0\n",
      "chat.py:249 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:10:55+00:00 Tokenizing dataset cherrypick_chat\n",
      "chat.py:269 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:11:01+00:00 Postprocessing tokenized data in cherrypick_chat\n",
      "base.py:48 [\u001b[1mINFO\u001b[0m] 2024-09-07T15:11:01+00:00 Sampled 2972 records with offset None\n",
      "  0%|                                                   | 0/370 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "{'loss': 0.3215, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.01}         \n",
      "{'loss': 0.333, 'grad_norm': 6.259866714477539, 'learning_rate': 8.333333333333333e-08, 'epoch': 0.01}\n",
      "{'loss': 0.3312, 'grad_norm': 7.341131687164307, 'learning_rate': 1.6666666666666665e-07, 'epoch': 0.02}\n",
      "{'loss': 0.3171, 'grad_norm': 5.021354675292969, 'learning_rate': 2.5e-07, 'epoch': 0.02}\n",
      "{'loss': 0.3275, 'grad_norm': 6.2353715896606445, 'learning_rate': 3.333333333333333e-07, 'epoch': 0.03}\n",
      "{'loss': 0.3579, 'grad_norm': 7.450692653656006, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.03}\n",
      "{'loss': 0.3686, 'grad_norm': 7.0388288497924805, 'learning_rate': 5e-07, 'epoch': 0.04}\n",
      "{'loss': 0.3823, 'grad_norm': 7.940568447113037, 'learning_rate': 5.833333333333334e-07, 'epoch': 0.04}\n",
      "{'loss': 0.2565, 'grad_norm': 4.52988862991333, 'learning_rate': 6.666666666666666e-07, 'epoch': 0.05}\n",
      "{'loss': 0.3812, 'grad_norm': 6.584277153015137, 'learning_rate': 7.5e-07, 'epoch': 0.05}\n",
      "{'loss': 0.3977, 'grad_norm': 7.454662799835205, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.06}\n",
      "{'loss': 0.3062, 'grad_norm': 5.995263576507568, 'learning_rate': 9.166666666666665e-07, 'epoch': 0.06}\n",
      "{'loss': 0.3517, 'grad_norm': 5.895185947418213, 'learning_rate': 1e-06, 'epoch': 0.07}\n",
      "{'loss': 0.3109, 'grad_norm': 5.6002278327941895, 'learning_rate': 9.972067039106144e-07, 'epoch': 0.08}\n",
      "{'loss': 0.3425, 'grad_norm': 5.301450729370117, 'learning_rate': 9.94413407821229e-07, 'epoch': 0.08}\n",
      "{'loss': 0.3947, 'grad_norm': 7.75302791595459, 'learning_rate': 9.916201117318436e-07, 'epoch': 0.09}\n",
      "{'loss': 0.3493, 'grad_norm': 6.992016792297363, 'learning_rate': 9.888268156424581e-07, 'epoch': 0.09}\n",
      "{'loss': 0.3613, 'grad_norm': 7.242837429046631, 'learning_rate': 9.860335195530726e-07, 'epoch': 0.1}\n",
      "{'loss': 0.3685, 'grad_norm': nan, 'learning_rate': 9.860335195530726e-07, 'epoch': 0.1}\n",
      "{'loss': 0.3812, 'grad_norm': 7.561788082122803, 'learning_rate': 9.83240223463687e-07, 'epoch': 0.11}\n",
      "{'loss': 0.3207, 'grad_norm': 6.604639053344727, 'learning_rate': 9.804469273743016e-07, 'epoch': 0.11}\n",
      "{'loss': 0.3494, 'grad_norm': 7.473615646362305, 'learning_rate': 9.776536312849163e-07, 'epoch': 0.12}\n",
      "{'loss': 0.345, 'grad_norm': 6.560888290405273, 'learning_rate': 9.748603351955308e-07, 'epoch': 0.12}\n",
      "{'loss': 0.301, 'grad_norm': 5.3697590827941895, 'learning_rate': 9.720670391061452e-07, 'epoch': 0.13}\n",
      "{'loss': 0.2495, 'grad_norm': 3.6452198028564453, 'learning_rate': 9.692737430167597e-07, 'epoch': 0.13}\n",
      "{'loss': 0.3058, 'grad_norm': 6.568215847015381, 'learning_rate': 9.664804469273742e-07, 'epoch': 0.14}\n",
      "{'loss': 0.2735, 'grad_norm': 5.367036819458008, 'learning_rate': 9.636871508379887e-07, 'epoch': 0.15}\n",
      "{'loss': 0.3165, 'grad_norm': 5.883930206298828, 'learning_rate': 9.608938547486032e-07, 'epoch': 0.15}\n",
      "{'loss': 0.2902, 'grad_norm': 5.257040977478027, 'learning_rate': 9.581005586592179e-07, 'epoch': 0.16}\n",
      "{'loss': 0.2772, 'grad_norm': 5.1581339836120605, 'learning_rate': 9.553072625698324e-07, 'epoch': 0.16}\n",
      "{'loss': 0.2504, 'grad_norm': 4.272290229797363, 'learning_rate': 9.525139664804469e-07, 'epoch': 0.17}\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–Ž                                    | 31/370 [06:28<1:09:52, 12.37s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3  TOKENIZERS_PARALLELISM=false python -m turbo_alignment train_sft --experiment_settings_path configs/t_bank_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1f0a7-b26a-4ba1-9e41-956cf8eb4a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
